{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c571ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==================================================================================\n",
    "# üöÄ CREDIT RISK CLASSIFICATION TRAINING - CONFIG DRIVEN\n",
    "# ==================================================================================\n",
    "# Fully integrated with pipeline_config.yml and config.yml\n",
    "# Supports Random Forest Classifier with preprocessing pipeline\n",
    "# Logs all experiments to MLflow and evaluates classification metrics\n",
    "# ==================================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "%pip install scikit-learn pyyaml\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ CREDIT RISK CLASSIFICATION TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================================================================================\n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION\n",
    "# ==================================================================================\n",
    "print(\"\\nüìã Step 1: Loading pipeline configuration...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Extract configuration values\n",
    "    MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "    CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "    SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "    BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "    \n",
    "    EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "    MODEL_ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "    RUN_NAME_PREFIX = pipeline_cfg[\"experiment\"][\"run_name_prefix\"]\n",
    "\n",
    "    # Data configuration\n",
    "    INPUT_TABLE = pipeline_cfg[\"data\"][\"input_table\"]\n",
    "    FEATURE_COLS = pipeline_cfg[\"data\"][\"features\"]\n",
    "    LABEL_COL = pipeline_cfg[\"data\"][\"label\"]\n",
    "    TEST_SIZE = pipeline_cfg[\"data\"][\"split\"][\"test_size\"]\n",
    "    RANDOM_STATE = pipeline_cfg[\"data\"][\"split\"][\"random_state\"]\n",
    "    STRATIFY = pipeline_cfg[\"data\"][\"split\"][\"stratify\"]\n",
    "\n",
    "    # Metrics configuration\n",
    "    METRICS_CONFIG = pipeline_cfg[\"metrics\"][\"classification\"]\n",
    "    PRIMARY_METRIC = METRICS_CONFIG[\"primary_metric\"]\n",
    "    DIRECTION = METRICS_CONFIG[\"direction\"]\n",
    "    TRACKED_METRICS = METRICS_CONFIG[\"tracked_metrics\"]\n",
    "    THRESHOLD_METRICS = METRICS_CONFIG[\"threshold_metrics\"]\n",
    "\n",
    "    print(f\"‚úÖ Pipeline configuration loaded successfully!\")\n",
    "    print(f\"\\nüìä Configuration Summary:\")\n",
    "    print(f\"   Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Input Table: {INPUT_TABLE}\")\n",
    "    print(f\"   Features: {len(FEATURE_COLS)} columns\")\n",
    "    print(f\"   Label: {LABEL_COL}\")\n",
    "    print(f\"   Primary Metric: {PRIMARY_METRIC}\")\n",
    "    print(f\"   Test Split: {TEST_SIZE * 100}%\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    print(\"üí° Please ensure pipeline_config.yml is in the notebook directory\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading pipeline configuration: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================================================================================\n",
    "# ‚úÖ LOAD EXPERIMENT CONFIGURATIONS (config.yml)\n",
    "# ==================================================================================\n",
    "def load_experiment_configs(path=\"config.yml\"):\n",
    "    \"\"\"Load Random Forest hyperparameter configurations\"\"\"\n",
    "    print(f\"\\nüìÑ Step 2: Loading experiment configurations from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        num_experiments = len(config[\"experiments\"])\n",
    "        print(f\"‚úÖ Found {num_experiments} experiment configuration(s):\")\n",
    "        \n",
    "        for i, exp in enumerate(config[\"experiments\"], 1):\n",
    "            print(f\"   {i}. {exp['name']}\")\n",
    "        \n",
    "        return config\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: {path} not found!\")\n",
    "        print(\"üí° Please create config.yml with Random Forest configurations\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR loading experiment configs: {e}\")\n",
    "        raise\n",
    "\n",
    "# ==================================================================================\n",
    "# ‚úÖ DATA LOADING FROM DELTA TABLE\n",
    "# ==================================================================================\n",
    "def load_data(spark):\n",
    "    \"\"\"\n",
    "    Load and prepare data from Delta table\n",
    "    Returns: X (features), y (labels) as pandas DataFrames\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì¶ Step 3: Loading data from Delta table...\")\n",
    "    print(f\"   Table: {INPUT_TABLE}\")\n",
    "    \n",
    "    try:\n",
    "        # Read from Delta\n",
    "        df = spark.read.format(\"delta\").table(INPUT_TABLE)\n",
    "        \n",
    "        # Select features and label\n",
    "        df_selected = df.select(*FEATURE_COLS, LABEL_COL)\n",
    "        \n",
    "        # Convert to pandas\n",
    "        df_pd = df_selected.toPandas()\n",
    "        \n",
    "        # Separate features and labels\n",
    "        X = df_pd[FEATURE_COLS]\n",
    "        y = df_pd[LABEL_COL]\n",
    "        \n",
    "        # Convert label to binary if string\n",
    "        if y.dtype == 'object':\n",
    "            y = (y == 'yes').astype(int)\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully!\")\n",
    "        print(f\"   Total samples: {len(df_pd):,}\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Label distribution:\")\n",
    "        print(f\"      Class 0 (No Default): {(y == 0).sum():,} ({(y == 0).sum() / len(y) * 100:.1f}%)\")\n",
    "        print(f\"      Class 1 (Default): {(y == 1).sum():,} ({(y == 1).sum() / len(y) * 100:.1f}%)\")\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load data from '{INPUT_TABLE}': {e}\")\n",
    "        print(\"üí° Verify the table exists and contains required columns\")\n",
    "        raise\n",
    "\n",
    "# ==================================================================================\n",
    "# ‚úÖ TRAIN SINGLE EXPERIMENT\n",
    "# ==================================================================================\n",
    "def train_single_experiment(X, y, params, run_name):\n",
    "    \"\"\"\n",
    "    Train a single Random Forest Classifier configuration\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target labels\n",
    "        params: Model hyperparameters from config.yml\n",
    "        run_name: Name for MLflow run\n",
    "    \n",
    "    Returns:\n",
    "        run_id: MLflow run ID\n",
    "        metrics_dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîÅ Training Experiment: {run_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìù Hyperparameters:\")\n",
    "    for k, v in params.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "    # Split data (stratified for imbalanced classes)\n",
    "    stratify_param = y if STRATIFY else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=TEST_SIZE, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=stratify_param\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä Data Split:\")\n",
    "    print(f\"   Train samples: {len(X_train):,}\")\n",
    "    print(f\"   Test samples: {len(X_test):,}\")\n",
    "    print(f\"   Train class 1: {(y_train == 1).sum():,} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)\")\n",
    "    print(f\"   Test class 1: {(y_test == 1).sum():,} ({(y_test == 1).sum() / len(y_test) * 100:.1f}%)\")\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"\\nüîñ MLflow Run ID: {run_id}\")\n",
    "\n",
    "        # Log configuration metadata\n",
    "        mlflow.log_param(\"model_type\", MODEL_TYPE)\n",
    "        mlflow.log_param(\"experiment_name\", run_name)\n",
    "        mlflow.log_param(\"timestamp\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Log all hyperparameters\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "        # Log data split info\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"stratified_split\", STRATIFY)\n",
    "\n",
    "        # Train model\n",
    "        print(f\"\\nüèãÔ∏è Training Random Forest Classifier...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Training completed in {train_time:.2f} seconds\")\n",
    "        mlflow.log_metric(\"train_time\", train_time)\n",
    "\n",
    "        # Make predictions\n",
    "        print(f\"\\nüìä Evaluating model...\")\n",
    "        \n",
    "        # Training predictions\n",
    "        start_time = time.time()\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = (time.time() - start_time) / len(X_test) * 1000  # ms per sample\n",
    "        \n",
    "        mlflow.log_metric(\"inference_time\", inference_time)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics_dict = {\n",
    "            # Training metrics\n",
    "            \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "            \n",
    "            # Test metrics\n",
    "            \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "            \"test_precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "            \"test_recall\": recall_score(y_test, y_test_pred, zero_division=0),\n",
    "            \"test_f1\": f1_score(y_test, y_test_pred, zero_division=0),\n",
    "            \"test_roc_auc\": roc_auc_score(y_test, y_test_pred_proba),\n",
    "            \n",
    "            # Performance metrics\n",
    "            \"train_time\": train_time,\n",
    "            \"inference_time\": inference_time\n",
    "        }\n",
    "\n",
    "        # Log all metrics to MLflow\n",
    "        for metric_name, metric_value in metrics_dict.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\n‚úÖ Evaluation Results:\")\n",
    "        print(f\"   {'Metric':<20} {'Train':<12} {'Test':<12}\")\n",
    "        print(f\"   {'-'*44}\")\n",
    "        print(f\"   {'Accuracy':<20} {metrics_dict['train_accuracy']:<12.4f} {metrics_dict['test_accuracy']:<12.4f}\")\n",
    "        print(f\"   {'Precision':<20} {'-':<12} {metrics_dict['test_precision']:<12.4f}\")\n",
    "        print(f\"   {'Recall':<20} {'-':<12} {metrics_dict['test_recall']:<12.4f}\")\n",
    "        print(f\"   {'F1 Score':<20} {'-':<12} {metrics_dict['test_f1']:<12.4f}\")\n",
    "        print(f\"   {'ROC-AUC':<20} {'-':<12} {metrics_dict['test_roc_auc']:<12.4f}\")\n",
    "        print(f\"\\n   Training Time: {train_time:.2f}s\")\n",
    "        print(f\"   Inference Time: {inference_time:.3f}ms/sample\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        print(f\"\\n   Confusion Matrix:\")\n",
    "        print(f\"   [[TN={cm[0,0]:<6} FP={cm[0,1]:<6}]\")\n",
    "        print(f\"    [FN={cm[1,0]:<6} TP={cm[1,1]:<6}]]\")\n",
    "\n",
    "        # Check threshold metrics\n",
    "        print(f\"\\nüéØ Threshold Check:\")\n",
    "        passes_thresholds = True\n",
    "        for metric_name, threshold_value in THRESHOLD_METRICS.items():\n",
    "            actual_value = metrics_dict.get(metric_name)\n",
    "            if actual_value is not None:\n",
    "                passes = actual_value >= threshold_value\n",
    "                status = \"‚úÖ\" if passes else \"‚ùå\"\n",
    "                print(f\"   {status} {metric_name}: {actual_value:.4f} (threshold: {threshold_value})\")\n",
    "                if not passes:\n",
    "                    passes_thresholds = False\n",
    "        \n",
    "        mlflow.log_param(\"passes_thresholds\", passes_thresholds)\n",
    "\n",
    "        # Create model signature\n",
    "        signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "        # Log model to MLflow\n",
    "        print(f\"\\nüíæ Logging model to MLflow...\")\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            artifact_path=MODEL_ARTIFACT_PATH,\n",
    "            signature=signature,\n",
    "            registered_model_name=None  # Will register later in evaluation script\n",
    "        )\n",
    "\n",
    "        # Log feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': FEATURE_COLS,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nüéØ Top 10 Most Important Features:\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                print(f\"   {row['feature']:<25} {row['importance']:.4f}\")\n",
    "            \n",
    "            # Log as artifact\n",
    "            importance_file = \"feature_importance.csv\"\n",
    "            feature_importance.to_csv(importance_file, index=False)\n",
    "            mlflow.log_artifact(importance_file)\n",
    "\n",
    "        print(f\"\\n‚úÖ Experiment '{run_name}' completed successfully!\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        return run_id, metrics_dict\n",
    "\n",
    "# ==================================================================================\n",
    "# ‚úÖ MAIN EXECUTION\n",
    "# ==================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Initialize MLflow\n",
    "    print(\"\\nüîß Initializing MLflow...\")\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(\"databricks\")\n",
    "        mlflow.set_registry_uri(\"databricks-uc\")\n",
    "        mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"‚úÖ MLflow experiment set: {EXPERIMENT_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize MLflow: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Initialize Spark\n",
    "    print(\"\\nüîß Initializing Spark...\")\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"CreditRiskTraining\").getOrCreate()\n",
    "        print(\"‚úÖ Spark session created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Load data\n",
    "    X, y = load_data(spark)\n",
    "\n",
    "    # Load experiment configurations\n",
    "    config = load_experiment_configs()\n",
    "\n",
    "    # Start training all experiments\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üöÄ STARTING TRAINING RUNS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    run_results = []\n",
    "    total_experiments = len(config[\"experiments\"])\n",
    "\n",
    "    for idx, exp in enumerate(config[\"experiments\"], 1):\n",
    "        exp_name = exp[\"name\"]\n",
    "        exp_params = exp[\"params\"]\n",
    "        \n",
    "        print(f\"\\n[Experiment {idx}/{total_experiments}]\")\n",
    "        \n",
    "        try:\n",
    "            run_id, metrics = train_single_experiment(\n",
    "                X, y, \n",
    "                exp_params, \n",
    "                run_name=f\"{RUN_NAME_PREFIX}_{exp_name}\"\n",
    "            )\n",
    "            \n",
    "            run_results.append({\n",
    "                'name': exp_name,\n",
    "                'run_id': run_id,\n",
    "                'metrics': metrics,\n",
    "                'params': exp_params\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to train {exp_name}: {e}\")\n",
    "            print(f\"   Continuing with next experiment...\")\n",
    "            continue\n",
    "\n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ‚úÖ‚úÖ ALL TRAINING RUNS COMPLETED ‚úÖ‚úÖ‚úÖ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if run_results:\n",
    "        print(f\"\\nüìä Training Summary ({len(run_results)}/{total_experiments} successful):\")\n",
    "        print(f\"\\n{'Rank':<6} {'Experiment':<35} {PRIMARY_METRIC.upper():<12} {'ROC-AUC':<10} {'Run ID':<40}\")\n",
    "        print(\"-\" * 103)\n",
    "        \n",
    "        # Sort by primary metric\n",
    "        if DIRECTION == \"maximize\":\n",
    "            sorted_results = sorted(run_results, key=lambda x: x['metrics'][PRIMARY_METRIC], reverse=True)\n",
    "        else:\n",
    "            sorted_results = sorted(run_results, key=lambda x: x['metrics'][PRIMARY_METRIC])\n",
    "        \n",
    "        for rank, result in enumerate(sorted_results, 1):\n",
    "            marker = \"üèÜ\" if rank == 1 else f\"{rank}.\"\n",
    "            name = result['name']\n",
    "            primary_score = result['metrics'][PRIMARY_METRIC]\n",
    "            roc_auc = result['metrics']['test_roc_auc']\n",
    "            run_id = result['run_id']\n",
    "            \n",
    "            print(f\"{marker:<6} {name:<35} {primary_score:<12.4f} {roc_auc:<10.4f} {run_id}\")\n",
    "        \n",
    "        # Highlight best model\n",
    "        best = sorted_results[0]\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üèÜ BEST MODEL FROM THIS TRAINING SESSION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Name: {best['name']}\")\n",
    "        print(f\"   {PRIMARY_METRIC.upper()}: {best['metrics'][PRIMARY_METRIC]:.4f}\")\n",
    "        print(f\"   ROC-AUC: {best['metrics']['test_roc_auc']:.4f}\")\n",
    "        print(f\"   Accuracy: {best['metrics']['test_accuracy']:.4f}\")\n",
    "        print(f\"   Recall: {best['metrics']['test_recall']:.4f}\")\n",
    "        print(f\"   Precision: {best['metrics']['test_precision']:.4f}\")\n",
    "        print(f\"   Run ID: {best['run_id']}\")\n",
    "        print(\"\\n   Key Hyperparameters:\")\n",
    "        for k in ['n_estimators', 'max_depth', 'min_samples_split', 'class_weight']:\n",
    "            if k in best['params']:\n",
    "                print(f\"      {k}: {best['params'][k]}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No successful training runs completed\")\n",
    "        print(\"üí° Check errors above and fix configurations\")\n",
    "\n",
    "    # Next steps\n",
    "    print(\"\\nüìå Next Steps:\")\n",
    "    print(\"   1. Run model_evaluation.py to evaluate ALL models\")\n",
    "    print(\"   2. Best model will be selected and registered\")\n",
    "    print(\"   3. Continue with UAT ‚Üí Production promotion\")\n",
    "    \n",
    "    print(f\"\\nüí° Note:\")\n",
    "    print(f\"   All {len(run_results)} models logged to: {EXPERIMENT_NAME}\")\n",
    "    print(f\"   Evaluation will compare based on: {PRIMARY_METRIC}\")\n",
    "    print(f\"   Models passing thresholds will be registered automatically\")\n",
    "    \n",
    "    # Save task values for workflow\n",
    "    try:\n",
    "        dbutils.jobs.taskValues.set(key=\"model_type\", value=MODEL_TYPE)\n",
    "        dbutils.jobs.taskValues.set(key=\"experiment_name\", value=EXPERIMENT_NAME)\n",
    "        dbutils.jobs.taskValues.set(key=\"num_experiments\", value=len(run_results))\n",
    "        dbutils.jobs.taskValues.set(key=\"primary_metric\", value=PRIMARY_METRIC)\n",
    "        \n",
    "        if run_results:\n",
    "            dbutils.jobs.taskValues.set(\n",
    "                key=\"best_score\", \n",
    "                value=float(sorted_results[0]['metrics'][PRIMARY_METRIC])\n",
    "            )\n",
    "            dbutils.jobs.taskValues.set(\n",
    "                key=\"best_run_id\",\n",
    "                value=sorted_results[0]['run_id']\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Task values saved for workflow automation\")\n",
    "        \n",
    "    except:\n",
    "        print(f\"\\n‚ÑπÔ∏è Not running in Databricks workflow - skipping task values\")\n",
    "    \n",
    "    print(\"\\nüéâ Training pipeline completed successfully!\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
