{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, round, regexp_replace, trim\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIG\n",
    "# =====================================================================\n",
    "INPUT_DELTA_TABLE = \"workspace.ml_credit_risk.credit_risk_data\"\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# SPARK INITIALIZATION\n",
    "# =====================================================================\n",
    "def initialize_spark():\n",
    "    if 'spark' in globals() and isinstance(globals()['spark'], SparkSession):\n",
    "        return globals()['spark']\n",
    "    return SparkSession.builder.appName(\"CreditRiskPreprocessing\").getOrCreate()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: DATA INGESTION\n",
    "# =====================================================================\n",
    "def ingest_data(spark: SparkSession, table_name: str):\n",
    "    print(f\"üì• Loading: {table_name}\")\n",
    "    \n",
    "    credit_df = spark.read.format(\"delta\").table(table_name)\n",
    "\n",
    "    feature_cols = [\n",
    "        'checking_balance', 'months_loan_duration', 'credit_history', 'purpose',\n",
    "        'amount', 'savings_balance', 'employment_duration', 'percent_of_income',\n",
    "        'years_at_residence', 'age', 'other_credit', 'housing',\n",
    "        'existing_loans_count', 'job', 'dependents', 'phone'\n",
    "    ]\n",
    "    \n",
    "    df = credit_df.select(*feature_cols, col(\"default\").alias(\"label\"))\n",
    "    print(f\"‚úÖ Loaded Rows: {df.count():,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: UNIT CLEANUP (KEEP SAME COLUMN NAMES)\n",
    "# =====================================================================\n",
    "def cleanup_units(df):\n",
    "    print(\"üßπ Cleaning units...\")\n",
    "\n",
    "    df = df.withColumn(\"checking_balance\", trim(regexp_replace(col(\"checking_balance\"), r\"\\s*DM\\s*$\", \"\")))\n",
    "    df = df.withColumn(\"savings_balance\", trim(regexp_replace(col(\"savings_balance\"), r\"\\s*DM\\s*$\", \"\")))\n",
    "    df = df.withColumn(\"employment_duration\", trim(regexp_replace(col(\"employment_duration\"), r\"\\s*years?\\s*$\", \"\")))\n",
    "\n",
    "    # Replace blanks with \"unknown\"\n",
    "    df = df.replace(\"\", \"unknown\")\n",
    "    \n",
    "    print(\"‚úÖ Unit cleanup done\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: DATA PREPARATION\n",
    "# =====================================================================\n",
    "def prepare_data(df):\n",
    "    print(\"üìä Preparing data...\")\n",
    "\n",
    "    df = df.withColumn(\"label\", when(col(\"label\") == \"yes\", 1.0).otherwise(0.0))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"monthly_income\",\n",
    "        round(\n",
    "            when(\n",
    "                (col(\"percent_of_income\") > 0) & (col(\"months_loan_duration\") > 0),\n",
    "                (col(\"amount\") / col(\"months_loan_duration\")) * (100 / col(\"percent_of_income\"))\n",
    "            ).otherwise(None),\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Preparation complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: ORDINAL ENCODING\n",
    "# =====================================================================\n",
    "def ordinal_encoding(df):\n",
    "    print(\"üî¢ Applying ordinal encoding...\")\n",
    "\n",
    "    ordinal_config = {\n",
    "        'checking_balance': ['< 0', '1 - 200', '> 200', 'unknown'],\n",
    "        'savings_balance': ['< 100', '100 - 500', '500 - 1000', '> 1000', 'unknown'],\n",
    "        'employment_duration': ['unemployed', '< 1', '1 - 4', '4 - 7', '> 7', 'unknown'],\n",
    "        'credit_history': ['critical', 'poor', 'good', 'very good', 'perfect']\n",
    "    }\n",
    "\n",
    "    for col_name, categories in ordinal_config.items():\n",
    "        expr = None\n",
    "        for idx, cat in enumerate(categories):\n",
    "            expr = when(col(col_name) == cat, float(idx)) if expr is None else expr.when(col(col_name) == cat, float(idx))\n",
    "        df = df.withColumn(col_name, expr.otherwise(float(len(categories))))\n",
    "\n",
    "    print(\"‚úÖ Ordinal encoding done\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: ONE-HOT ENCODING (FIXED FOR DATABRICKS REPOS)\n",
    "# =====================================================================\n",
    "def onehot_encoding(df):\n",
    "    print(\"üî• Applying Repo-safe OneHot Encoding...\")\n",
    "\n",
    "    nominal_cols = ['purpose', 'other_credit', 'housing', 'job', 'phone']\n",
    "\n",
    "    # FIX: Apply transformations sequentially without Pipeline\n",
    "    # This avoids Databricks Repos security restrictions\n",
    "    \n",
    "    # Step 1: StringIndexer for each column\n",
    "    for col_name in nominal_cols:\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=col_name,\n",
    "            outputCol=f\"{col_name}_index\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        df = indexer.fit(df).transform(df)\n",
    "    \n",
    "    # Step 2: OneHotEncoder for all indexed columns\n",
    "    index_cols = [f\"{c}_index\" for c in nominal_cols]\n",
    "    vec_cols = [f\"{c}_vec\" for c in nominal_cols]\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=index_cols,\n",
    "        outputCols=vec_cols,\n",
    "        dropLast=True\n",
    "    )\n",
    "    df = encoder.fit(df).transform(df)\n",
    "    \n",
    "    # Remove original nominal + index columns, keep only vectors\n",
    "    df = df.drop(*nominal_cols, *index_cols)\n",
    "\n",
    "    print(\"‚úÖ One-hot encoding complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: STANDARD SCALING\n",
    "# =====================================================================\n",
    "def apply_standard_scaling(df):\n",
    "    print(\"üìè Scaling features...\")\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c != \"label\"]\n",
    "\n",
    "    # Step 1: Assemble features\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"unscaled_features\")\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    # Step 2: Scale features\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"unscaled_features\",\n",
    "        outputCol=\"features\",\n",
    "        withMean=True,\n",
    "        withStd=True\n",
    "    )\n",
    "    scaler_model = scaler.fit(df)\n",
    "    df = scaler_model.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "    print(\"‚úÖ Scaling complete\")\n",
    "    return df, scaler_model\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ CREDIT RISK PIPELINE STARTING\\n\")\n",
    "    \n",
    "    spark = initialize_spark()\n",
    "\n",
    "    df = ingest_data(spark, INPUT_DELTA_TABLE)\n",
    "    df = cleanup_units(df)\n",
    "    df = prepare_data(df)\n",
    "    df = ordinal_encoding(df)\n",
    "    df = onehot_encoding(df)\n",
    "    processed_df, scaler_model = apply_standard_scaling(df)\n",
    "\n",
    "    print(\"\\nüéâ Pipeline complete!\")\n",
    "    print(f\"Rows: {processed_df.count():,}\")\n",
    "    processed_df.printSchema()\n",
    "    processed_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
