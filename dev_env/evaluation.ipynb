{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a034828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ MODEL EVALUATION SCRIPT â€” MULTI MODEL\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ MODEL EVALUATION PIPELINE â€” MULTI MODEL + UNIQUE EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 1ï¸âƒ£ Load Config\n",
    "# -------------------------\n",
    "print(\"\\nðŸ“‹ Loading configuration files...\")\n",
    "\n",
    "try:\n",
    "    # Load pipeline config\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # âœ… NEW: Load experiments config (for reference if needed)\n",
    "    with open(\"experiments_config.yml\", \"r\") as f:\n",
    "        experiments_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"âœ… Configuration Loaded\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# âœ… CHANGED: Parse comma-separated model types from Git variable\n",
    "MODEL_TYPES_RAW = pipeline_cfg[\"models\"][\"enabled\"]\n",
    "MODEL_TYPES = [m.strip() for m in MODEL_TYPES_RAW.split(\",\")]  # Split and clean\n",
    "\n",
    "print(f\"ðŸ“‹ Models to evaluate: {MODEL_TYPES}\\n\")\n",
    "\n",
    "UC_CATALOG = pipeline_cfg[\"models\"][\"catalog\"]\n",
    "UC_SCHEMA = pipeline_cfg[\"models\"][\"schema\"]\n",
    "BASE_NAME = pipeline_cfg[\"models\"][\"base_name\"]\n",
    "NAMING_FMT = pipeline_cfg[\"models\"][\"naming\"][\"format\"]\n",
    "\n",
    "EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "metrics_cfg = pipeline_cfg[\"metrics\"][\"classification\"]\n",
    "PRIMARY_METRIC = metrics_cfg[\"primary_metric\"]\n",
    "TRACKED_METRICS = metrics_cfg[\"tracked_metrics\"]\n",
    "DIRECTION = metrics_cfg[\"direction\"]\n",
    "\n",
    "EVALUATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "\n",
    "# -------------------------\n",
    "# 2ï¸âƒ£ Initialize MLflow & Spark\n",
    "# -------------------------\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluationMultiModel\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        raise Exception(f\"Experiment not found: {EXPERIMENT_NAME}\")\n",
    "\n",
    "    print(\"ðŸ”¥ MLflow + Spark loaded successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MLflow Init Failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# -------------------------\n",
    "# TABLE SCHEMA\n",
    "# -------------------------\n",
    "def get_evaluation_table_schema():\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_name\", StringType(), True),\n",
    "        StructField(\"model_type\", StringType(), True),  # âœ… NEW: Track model type explicitly\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"primary_metric\", StringType(), True),\n",
    "        StructField(\"primary_metric_value\", DoubleType(), True),\n",
    "        StructField(\"all_metrics_json\", StringType(), True),\n",
    "        StructField(\"params_json\", StringType(), True),\n",
    "        StructField(\"model_uri\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "# -------------------------\n",
    "# TABLE CREATION\n",
    "# -------------------------\n",
    "def ensure_table_exists():\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {EVALUATION_LOG_TABLE}\")\n",
    "        print(f\"âœ… Table exists: {EVALUATION_LOG_TABLE}\")\n",
    "    except:\n",
    "        print(f\"ðŸ“ Creating new table: {EVALUATION_LOG_TABLE}\")\n",
    "        empty_df = spark.createDataFrame([], get_evaluation_table_schema())\n",
    "        empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(EVALUATION_LOG_TABLE)\n",
    "        print(f\"âœ… Table created: {EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "# -------------------------\n",
    "# EXISTING RUN NAMES\n",
    "# -------------------------\n",
    "def get_existing_run_names():\n",
    "    try:\n",
    "        df = spark.sql(f\"SELECT DISTINCT run_name FROM {EVALUATION_LOG_TABLE}\")\n",
    "        existing = set(r.run_name for r in df.collect())\n",
    "        print(f\"ðŸ“Š Found {len(existing)} existing runs in evaluation table\")\n",
    "        return existing\n",
    "    except:\n",
    "        print(\"ðŸ“Š No existing runs found (table is empty)\")\n",
    "        return set()\n",
    "\n",
    "# -------------------------\n",
    "# FETCH RUNS (MODEL-WISE)\n",
    "# -------------------------\n",
    "def get_runs_for_model(model_type):\n",
    "    \"\"\"\n",
    "    Fetch runs for a specific model type from MLflow.\n",
    "    Uses run_name pattern matching to filter by model_type.\n",
    "    \"\"\"\n",
    "    order = f\"metrics.{PRIMARY_METRIC} {'DESC' if DIRECTION=='maximize' else 'ASC'}\"\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[order],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    # ðŸ”¥ FILTER ONLY THIS MODEL TYPE\n",
    "    # Training script creates run names like: credit_risk_random_forest_rf_baseline_balanced\n",
    "    # So we check if model_type is in the run_name\n",
    "    filtered_runs = [\n",
    "        run for run in runs\n",
    "        if model_type in (run.info.run_name or \"\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"   ðŸ” Found {len(filtered_runs)} runs for {model_type}\")\n",
    "    return filtered_runs\n",
    "\n",
    "# -------------------------\n",
    "# MAIN EVALUATION LOOP\n",
    "# -------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main evaluation logic:\n",
    "    1. For each model type (from Git variable)\n",
    "    2. Fetch all runs for that model\n",
    "    3. Keep only best run per unique experiment name\n",
    "    4. Log new runs to evaluation table\n",
    "    \"\"\"\n",
    "    \n",
    "    ensure_table_exists()\n",
    "    existing_run_names = get_existing_run_names()\n",
    "\n",
    "    all_records = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸš€ STARTING EVALUATION FOR ALL MODEL TYPES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for model_type in MODEL_TYPES:\n",
    "\n",
    "        print(f\"\\nðŸ“Š Evaluating model type: {model_type.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Generate full model name for UC registry\n",
    "        model_name = NAMING_FMT.format(\n",
    "            catalog=UC_CATALOG,\n",
    "            schema=UC_SCHEMA,\n",
    "            base_name=BASE_NAME,\n",
    "            model_type=model_type\n",
    "        )\n",
    "        \n",
    "        print(f\"   ðŸ“¦ Model Name: {model_name}\")\n",
    "\n",
    "        # Fetch all runs for this model type\n",
    "        runs = get_runs_for_model(model_type)\n",
    "\n",
    "        if not runs:\n",
    "            print(f\"   âš ï¸  No runs found for {model_type}\")\n",
    "            continue\n",
    "\n",
    "        # Dictionary to store best run per experiment name\n",
    "        run_dict = {}\n",
    "\n",
    "        for run in runs:\n",
    "            run_name = run.info.run_name or \"unnamed_run\"\n",
    "\n",
    "            # Skip if already logged\n",
    "            if run_name in existing_run_names:\n",
    "                continue\n",
    "\n",
    "            # Get primary metric value\n",
    "            pm_val = run.data.metrics.get(PRIMARY_METRIC)\n",
    "            \n",
    "            if pm_val is None:\n",
    "                continue\n",
    "\n",
    "            # Keep only best run per experiment name\n",
    "            if run_name not in run_dict or (\n",
    "                DIRECTION == \"maximize\" and pm_val > run_dict[run_name][\"primary_metric\"]\n",
    "            ):\n",
    "                run_dict[run_name] = {\n",
    "                    \"run_id\": run.info.run_id,\n",
    "                    \"run_name\": run_name,\n",
    "                    \"primary_metric\": pm_val,\n",
    "                    \"all_metrics\": {\n",
    "                        m: run.data.metrics.get(m)\n",
    "                        for m in TRACKED_METRICS\n",
    "                        if m in run.data.metrics\n",
    "                    },\n",
    "                    \"params\": run.data.params,\n",
    "                    \"model_uri\": f\"runs:/{run.info.run_id}/{ARTIFACT_PATH}\"\n",
    "                }\n",
    "\n",
    "        # Convert to records for DataFrame\n",
    "        for r in run_dict.values():\n",
    "            all_records.append({\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"run_id\": r[\"run_id\"],\n",
    "                \"run_name\": r[\"run_name\"],\n",
    "                \"model_type\": model_type,  # âœ… NEW: Add model type\n",
    "                \"model_name\": model_name,\n",
    "                \"primary_metric\": PRIMARY_METRIC,\n",
    "                \"primary_metric_value\": float(r[\"primary_metric\"]) if r[\"primary_metric\"] else 0.0,\n",
    "                \"all_metrics_json\": json.dumps(r[\"all_metrics\"]),\n",
    "                \"params_json\": json.dumps(r[\"params\"]),\n",
    "                \"model_uri\": r[\"model_uri\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"   âœ… Found {len(run_dict)} new unique experiments for {model_type}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE TO DELTA TABLE\n",
    "    # -------------------------\n",
    "    \n",
    "    if not all_records:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ… No new experiments to log (all runs already evaluated)\")\n",
    "        print(\"=\"*80)\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸ’¾ SAVING {len(all_records)} NEW EVALUATIONS TO TABLE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    spark_df = spark.createDataFrame(df, schema=get_evaluation_table_schema())\n",
    "\n",
    "    spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(EVALUATION_LOG_TABLE)\n",
    "\n",
    "    print(f\"\\nâœ… Successfully logged {len(all_records)} new evaluated experiments\")\n",
    "    print(f\"ðŸ“Š Table: {EVALUATION_LOG_TABLE}\")\n",
    "    \n",
    "    # Show summary by model type\n",
    "    print(\"\\nðŸ“‹ Summary by Model Type:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_type in MODEL_TYPES:\n",
    "        count = sum(1 for r in all_records if r[\"model_type\"] == model_type)\n",
    "        if count > 0:\n",
    "            print(f\"   {model_type}: {count} experiments\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸŽ‰ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
