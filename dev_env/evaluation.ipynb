{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a034828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ MODEL EVALUATION SCRIPT \n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ MODEL EVALUATION PIPELINE â€” UNIQUE EXPERIMENTS ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 1ï¸âƒ£ Load Config\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\nðŸ“‹ Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"âœ… Configuration Loaded\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, cfg):\n",
    "        self.MODEL_TYPE = cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{self.MODEL_TYPE}\"\n",
    "\n",
    "        self.EXPERIMENT_NAME = cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        metrics_cfg = cfg[\"metrics\"][\"classification\"]\n",
    "        self.PRIMARY_METRIC = metrics_cfg[\"primary_metric\"]\n",
    "        self.TRACKED_METRICS = metrics_cfg[\"tracked_metrics\"]\n",
    "        self.DIRECTION = metrics_cfg[\"direction\"]\n",
    "        self.THRESHOLD_METRICS = metrics_cfg[\"threshold_metrics\"]\n",
    "\n",
    "        self.EVALUATION_LOG_TABLE = cfg[\"tables\"][\"evaluation_log\"]\n",
    "        self.RECENT_N = cfg[\"comparison\"][\"recent_n\"]\n",
    "\n",
    "        print(f\"\\nðŸ“Œ Evaluation Config Summary:\")\n",
    "        print(f\"   Model: {self.MODEL_NAME}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC} ({self.DIRECTION})\")\n",
    "        print(f\"   Logging Table: {self.EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "config = Config(pipeline_cfg)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 2ï¸âƒ£ Initialize MLflow & Spark\n",
    "# -------------------------\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluationOnly\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        raise Exception(f\"Experiment not found: {config.EXPERIMENT_NAME}\")\n",
    "\n",
    "    print(\"\\nðŸ”¥ MLflow + Spark loaded successfully\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MLflow Init Failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TABLE SCHEMA DEFINITION\n",
    "# -------------------------\n",
    "\n",
    "def get_evaluation_table_schema():\n",
    "    \"\"\"Define fixed schema for evaluation log table\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_name\", StringType(), True),\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"primary_metric\", StringType(), True),\n",
    "        StructField(\"primary_metric_value\", DoubleType(), True),\n",
    "        StructField(\"all_metrics_json\", StringType(), True),\n",
    "        StructField(\"params_json\", StringType(), True),\n",
    "        StructField(\"model_uri\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AUTO TABLE CREATION\n",
    "# -------------------------\n",
    "\n",
    "def ensure_table_exists():\n",
    "    \"\"\"Create evaluation delta table if not exists\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {config.EVALUATION_LOG_TABLE}\")\n",
    "        print(f\"ðŸ“Œ Table exists: {config.EVALUATION_LOG_TABLE}\")\n",
    "    except:\n",
    "        print(f\"ðŸ†• Creating new Delta table: {config.EVALUATION_LOG_TABLE}\")\n",
    "        schema = get_evaluation_table_schema()\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "        print(f\"âœ… Table created: {config.EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CHECK IF ALREADY LOGGED (BASED ON RUN_NAME)\n",
    "# -------------------------\n",
    "\n",
    "def get_existing_run_names() -> set:\n",
    "    \"\"\"Get all run_names already in the evaluation table\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.sql(f\"\"\"\n",
    "            SELECT DISTINCT run_name \n",
    "            FROM {config.EVALUATION_LOG_TABLE}\n",
    "        \"\"\")\n",
    "        existing_names = set([row.run_name for row in existing_df.collect()])\n",
    "        print(f\"ðŸ“Š Found {len(existing_names)} unique experiments in evaluation table\")\n",
    "        return existing_names\n",
    "    except:\n",
    "        print(\"ðŸ“Š No existing entries found (new table)\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3ï¸âƒ£ Fetch Runs (BY UNIQUE RUN_NAME)\n",
    "# -------------------------\n",
    "\n",
    "def get_recent_runs():\n",
    "    print(\"\\nðŸ“ Fetching Experiment Runs...\")\n",
    "\n",
    "    order = f\"metrics.{config.PRIMARY_METRIC} {'DESC' if config.DIRECTION=='maximize' else 'ASC'}\"\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[order],\n",
    "        max_results=500  # Fetch more to ensure we get all experiments\n",
    "    )\n",
    "\n",
    "    if not runs:\n",
    "        print(\"âš  No model runs found.\")\n",
    "        return []\n",
    "\n",
    "    # ðŸ”¥ Get existing run_names (not run_ids) to filter out duplicates\n",
    "    existing_run_names = get_existing_run_names()\n",
    "\n",
    "    run_dict = {}  # Use dict to keep only best version of each experiment\n",
    "    skipped_count = 0\n",
    "\n",
    "    for run in runs:\n",
    "        run_name = run.info.run_name or \"unnamed_run\"\n",
    "        \n",
    "        # ðŸ”¥ Skip if already logged in table\n",
    "        if run_name in existing_run_names:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # ðŸ”¥ Keep only the BEST run for each experiment name\n",
    "        primary_metric_val = run.data.metrics.get(config.PRIMARY_METRIC)\n",
    "        \n",
    "        if run_name not in run_dict:\n",
    "            # First time seeing this experiment\n",
    "            run_dict[run_name] = {\n",
    "                \"run_id\": run.info.run_id,\n",
    "                \"run_name\": run_name,\n",
    "                \"primary_metric\": primary_metric_val,\n",
    "                \"all_metrics\": {m: run.data.metrics.get(m) for m in config.TRACKED_METRICS if m in run.data.metrics},\n",
    "                \"params\": run.data.params,\n",
    "                \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\",\n",
    "                \"timestamp\": datetime.fromtimestamp(run.info.start_time / 1000),\n",
    "            }\n",
    "        else:\n",
    "            # We've seen this experiment before, keep the better one\n",
    "            if config.DIRECTION == \"maximize\":\n",
    "                if primary_metric_val > run_dict[run_name][\"primary_metric\"]:\n",
    "                    run_dict[run_name] = {\n",
    "                        \"run_id\": run.info.run_id,\n",
    "                        \"run_name\": run_name,\n",
    "                        \"primary_metric\": primary_metric_val,\n",
    "                        \"all_metrics\": {m: run.data.metrics.get(m) for m in config.TRACKED_METRICS if m in run.data.metrics},\n",
    "                        \"params\": run.data.params,\n",
    "                        \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\",\n",
    "                        \"timestamp\": datetime.fromtimestamp(run.info.start_time / 1000),\n",
    "                    }\n",
    "            else:  # minimize\n",
    "                if primary_metric_val < run_dict[run_name][\"primary_metric\"]:\n",
    "                    run_dict[run_name] = {\n",
    "                        \"run_id\": run.info.run_id,\n",
    "                        \"run_name\": run_name,\n",
    "                        \"primary_metric\": primary_metric_val,\n",
    "                        \"all_metrics\": {m: run.data.metrics.get(m) for m in config.TRACKED_METRICS if m in run.data.metrics},\n",
    "                        \"params\": run.data.params,\n",
    "                        \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\",\n",
    "                        \"timestamp\": datetime.fromtimestamp(run.info.start_time / 1000),\n",
    "                    }\n",
    "\n",
    "    run_list = list(run_dict.values())\n",
    "    print(f\"ðŸ“Œ {len(run_list)} NEW unique experiments to log (Skipped {skipped_count} already in table)\")\n",
    "    return run_list\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4ï¸âƒ£ Log Evaluation Results (NO DUPLICATES)\n",
    "# -------------------------\n",
    "\n",
    "def log_results(run_list):\n",
    "    if not run_list:\n",
    "        print(\"\\nâœ… No new experiments to log (all already exist)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ“ Logging {len(run_list)} unique experiments...\")\n",
    "\n",
    "    records = []\n",
    "    for run in run_list:\n",
    "        records.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"run_id\": run[\"run_id\"],\n",
    "            \"run_name\": run[\"run_name\"],\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": float(run[\"primary_metric\"]) if run[\"primary_metric\"] else 0.0,\n",
    "            \"all_metrics_json\": json.dumps(run[\"all_metrics\"]),\n",
    "            \"params_json\": json.dumps(run[\"params\"]),\n",
    "            \"model_uri\": run[\"model_uri\"]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    spark_df = spark.createDataFrame(df, schema=get_evaluation_table_schema())\n",
    "\n",
    "    try:\n",
    "        # Append only new unique experiments\n",
    "        spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "        print(f\"âœ… Successfully logged {len(run_list)} unique experiments\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to log results: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5ï¸âƒ£ Display Summary\n",
    "# -------------------------\n",
    "\n",
    "def show_summary(run_list):\n",
    "    if not run_list:\n",
    "        print(\"\\nðŸ“Š No new experiments to display\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nðŸ“Š NEWLY LOGGED UNIQUE EXPERIMENTS:\\n\")\n",
    "    for rank, run in enumerate(run_list, 1):\n",
    "        pm_val = run['primary_metric']\n",
    "        print(f\"{rank}. {run['run_name']} â†’ {config.PRIMARY_METRIC}: {pm_val:.4f if pm_val else 0.0}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6ï¸âƒ£ Show All Unique Experiments\n",
    "# -------------------------\n",
    "\n",
    "def show_all_experiments():\n",
    "    \"\"\"Display all unique experiments in the evaluation table\"\"\"\n",
    "    print(\"\\nðŸ“‹ ALL UNIQUE EXPERIMENTS IN EVALUATION TABLE:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # ðŸ”¥ FIX: Select DISTINCT run_name to show only unique experiments\n",
    "        all_runs = spark.sql(f\"\"\"\n",
    "            SELECT run_name, primary_metric_value, MAX(timestamp) as latest_timestamp\n",
    "            FROM {config.EVALUATION_LOG_TABLE}\n",
    "            GROUP BY run_name, primary_metric_value\n",
    "            ORDER BY primary_metric_value DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        results = all_runs.collect()\n",
    "        \n",
    "        for idx, row in enumerate(results, 1):\n",
    "            print(f\"{idx}. {row.run_name} â†’ {config.PRIMARY_METRIC}: {row.primary_metric_value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Total Unique Experiments: {len(results)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not fetch experiments: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ðŸš€ MAIN EXECUTION\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    # Ensure table exists\n",
    "    ensure_table_exists()\n",
    "    \n",
    "    # Fetch new unique experiments (excluding already logged)\n",
    "    run_list = get_recent_runs()\n",
    "    \n",
    "    # Log new results\n",
    "    log_results(run_list)\n",
    "    \n",
    "    # Show summary of newly logged\n",
    "    show_summary(run_list)\n",
    "    \n",
    "    # Show all unique experiments\n",
    "    show_all_experiments()\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Evaluation Completed Successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
