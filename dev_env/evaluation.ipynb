{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a034828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL EVALUATION SCRIPT \n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL EVALUATION PIPELINE ‚Äî LOG ONLY (NO REGISTRATION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 1Ô∏è‚É£ Load Config\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\nüìã Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Configuration Loaded\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, cfg):\n",
    "        self.MODEL_TYPE = cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{self.MODEL_TYPE}\"\n",
    "\n",
    "        self.EXPERIMENT_NAME = cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        metrics_cfg = cfg[\"metrics\"][\"classification\"]\n",
    "        self.PRIMARY_METRIC = metrics_cfg[\"primary_metric\"]\n",
    "        self.TRACKED_METRICS = metrics_cfg[\"tracked_metrics\"]\n",
    "        self.DIRECTION = metrics_cfg[\"direction\"]\n",
    "        self.THRESHOLD_METRICS = metrics_cfg[\"threshold_metrics\"]\n",
    "\n",
    "        self.EVALUATION_LOG_TABLE = cfg[\"tables\"][\"evaluation_log\"]\n",
    "        self.RECENT_N = cfg[\"comparison\"][\"recent_n\"]\n",
    "\n",
    "        print(f\"\\nüìå Evaluation Config Summary:\")\n",
    "        print(f\"   Model: {self.MODEL_NAME}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC} ({self.DIRECTION})\")\n",
    "        print(f\"   Logging Table: {self.EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "config = Config(pipeline_cfg)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 2Ô∏è‚É£ Initialize MLflow & Spark\n",
    "# -------------------------\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluationOnly\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        raise Exception(f\"Experiment not found: {config.EXPERIMENT_NAME}\")\n",
    "\n",
    "    print(\"\\nüî• MLflow + Spark loaded successfully\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MLflow Init Failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# -------------------------\n",
    "# 3Ô∏è‚É£ Fetch Runs\n",
    "# -------------------------\n",
    "\n",
    "def get_recent_runs():\n",
    "    print(\"\\nüìç Fetching Experiment Runs...\")\n",
    "\n",
    "    order = (\n",
    "        f\"metrics.{config.PRIMARY_METRIC} DESC\"\n",
    "        if config.DIRECTION == \"maximize\"\n",
    "        else f\"metrics.{config.PRIMARY_METRIC} ASC\"\n",
    "    )\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[order],\n",
    "        max_results=config.RECENT_N\n",
    "    )\n",
    "\n",
    "    if not runs:\n",
    "        print(\"‚ö† No model runs found.\")\n",
    "        return []\n",
    "\n",
    "    run_list = []\n",
    "    for run in runs:\n",
    "        metrics = {m: run.data.metrics.get(m) for m in config.TRACKED_METRICS}\n",
    "\n",
    "        run_list.append({\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"run_name\": run.info.run_name,\n",
    "            \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"all_metrics\": metrics,\n",
    "            \"params\": run.data.params,\n",
    "            \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\",\n",
    "            \"timestamp\": datetime.fromtimestamp(run.info.start_time / 1000),\n",
    "        })\n",
    "\n",
    "    print(f\"üìå {len(run_list)} runs fetched and ready for logging.\")\n",
    "    return run_list\n",
    "\n",
    "# -------------------------\n",
    "# 4Ô∏è‚É£ Log Evaluation Results\n",
    "# -------------------------\n",
    "\n",
    "def log_results(run_list):\n",
    "    print(\"\\nüìù Logging evaluation results to Delta table...\")\n",
    "\n",
    "    records = []\n",
    "    for run in run_list:\n",
    "        records.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"run_id\": run[\"run_id\"],\n",
    "            \"run_name\": run[\"run_name\"],\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": run[\"primary_metric\"],\n",
    "            \"all_metrics_json\": json.dumps(run[\"all_metrics\"]),\n",
    "            \"params_json\": json.dumps(run[\"params\"]),\n",
    "            \"model_uri\": run[\"model_uri\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    spark.createDataFrame(df).write.format(\"delta\").mode(\"append\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "    print(\"‚úÖ Evaluation logged successfully.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5Ô∏è‚É£ Display Summary\n",
    "# -------------------------\n",
    "\n",
    "def show_summary(run_list):\n",
    "    print(\"\\nüìä TOP MODEL RESULTS:\\n\")\n",
    "    for rank, run in enumerate(run_list[:10], 1):\n",
    "        print(f\"{rank}. {run['run_name']} ‚Üí {config.PRIMARY_METRIC}: {run['primary_metric']:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# üöÄ MAIN EXECUTION\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    run_list = get_recent_runs()\n",
    "    if not run_list:\n",
    "        print(\"‚ùå No runs found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    log_results(run_list)\n",
    "    show_summary(run_list)\n",
    "\n",
    "    print(\"\\nüéâ Evaluation Completed ‚Äî No Models Registered (As Expected)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
