{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a034828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL EVALUATION SCRIPT \n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL EVALUATION PIPELINE ‚Äî UNIQUE ENTRIES ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 1Ô∏è‚É£ Load Config\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\nüìã Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Configuration Loaded\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, cfg):\n",
    "        self.MODEL_TYPE = cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{self.MODEL_TYPE}\"\n",
    "\n",
    "        self.EXPERIMENT_NAME = cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        metrics_cfg = cfg[\"metrics\"][\"classification\"]\n",
    "        self.PRIMARY_METRIC = metrics_cfg[\"primary_metric\"]\n",
    "        self.TRACKED_METRICS = metrics_cfg[\"tracked_metrics\"]\n",
    "        self.DIRECTION = metrics_cfg[\"direction\"]\n",
    "        self.THRESHOLD_METRICS = metrics_cfg[\"threshold_metrics\"]\n",
    "\n",
    "        self.EVALUATION_LOG_TABLE = cfg[\"tables\"][\"evaluation_log\"]\n",
    "        self.RECENT_N = cfg[\"comparison\"][\"recent_n\"]\n",
    "\n",
    "        print(f\"\\nüìå Evaluation Config Summary:\")\n",
    "        print(f\"   Model: {self.MODEL_NAME}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC} ({self.DIRECTION})\")\n",
    "        print(f\"   Logging Table: {self.EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "config = Config(pipeline_cfg)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 2Ô∏è‚É£ Initialize MLflow & Spark\n",
    "# -------------------------\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelEvaluationOnly\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        raise Exception(f\"Experiment not found: {config.EXPERIMENT_NAME}\")\n",
    "\n",
    "    print(\"\\nüî• MLflow + Spark loaded successfully\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MLflow Init Failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TABLE SCHEMA DEFINITION\n",
    "# -------------------------\n",
    "\n",
    "def get_evaluation_table_schema():\n",
    "    \"\"\"Define fixed schema for evaluation log table\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_name\", StringType(), True),\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"primary_metric\", StringType(), True),\n",
    "        StructField(\"primary_metric_value\", DoubleType(), True),\n",
    "        StructField(\"all_metrics_json\", StringType(), True),\n",
    "        StructField(\"params_json\", StringType(), True),\n",
    "        StructField(\"model_uri\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AUTO TABLE CREATION\n",
    "# -------------------------\n",
    "\n",
    "def ensure_table_exists():\n",
    "    \"\"\"Create evaluation delta table if not exists\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {config.EVALUATION_LOG_TABLE}\")\n",
    "        print(f\"üìå Table exists: {config.EVALUATION_LOG_TABLE}\")\n",
    "    except:\n",
    "        print(f\"üÜï Creating new Delta table: {config.EVALUATION_LOG_TABLE}\")\n",
    "        schema = get_evaluation_table_schema()\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "        print(f\"‚úÖ Table created: {config.EVALUATION_LOG_TABLE}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CHECK IF ALREADY LOGGED\n",
    "# -------------------------\n",
    "\n",
    "def get_existing_run_ids() -> set:\n",
    "    \"\"\"Get all run_ids already in the evaluation table\"\"\"\n",
    "    try:\n",
    "        existing_df = spark.sql(f\"\"\"\n",
    "            SELECT DISTINCT run_id \n",
    "            FROM {config.EVALUATION_LOG_TABLE}\n",
    "        \"\"\")\n",
    "        existing_ids = set([row.run_id for row in existing_df.collect()])\n",
    "        print(f\"üìä Found {len(existing_ids)} existing entries in evaluation table\")\n",
    "        return existing_ids\n",
    "    except:\n",
    "        print(\"üìä No existing entries found (new table)\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3Ô∏è‚É£ Fetch Runs\n",
    "# -------------------------\n",
    "\n",
    "def get_recent_runs():\n",
    "    print(\"\\nüìç Fetching Experiment Runs...\")\n",
    "\n",
    "    order = f\"metrics.{config.PRIMARY_METRIC} {'DESC' if config.DIRECTION=='maximize' else 'ASC'}\"\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[order],\n",
    "        max_results=config.RECENT_N\n",
    "    )\n",
    "\n",
    "    if not runs:\n",
    "        print(\"‚ö† No model runs found.\")\n",
    "        return []\n",
    "\n",
    "    # Get existing run_ids to filter out duplicates\n",
    "    existing_run_ids = get_existing_run_ids()\n",
    "\n",
    "    run_list = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for run in runs:\n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "        # üî• Skip if already logged\n",
    "        if run_id in existing_run_ids:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        metrics = {m: run.data.metrics.get(m) for m in config.TRACKED_METRICS if m in run.data.metrics}\n",
    "\n",
    "        run_list.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "            \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"all_metrics\": metrics,\n",
    "            \"params\": run.data.params,\n",
    "            \"model_uri\": f\"runs:/{run_id}/{config.ARTIFACT_PATH}\",\n",
    "            \"timestamp\": datetime.fromtimestamp(run.info.start_time / 1000),\n",
    "        })\n",
    "\n",
    "    print(f\"üìå {len(run_list)} NEW runs to log (Skipped {skipped_count} already logged)\")\n",
    "    return run_list\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4Ô∏è‚É£ Log Evaluation Results (NO DUPLICATES)\n",
    "# -------------------------\n",
    "\n",
    "def log_results(run_list):\n",
    "    if not run_list:\n",
    "        print(\"\\n‚úÖ No new runs to log (all already exist)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüìù Logging {len(run_list)} new evaluation results...\")\n",
    "\n",
    "    records = []\n",
    "    for run in run_list:\n",
    "        records.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"run_id\": run[\"run_id\"],\n",
    "            \"run_name\": run[\"run_name\"],\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": float(run[\"primary_metric\"]) if run[\"primary_metric\"] else 0.0,\n",
    "            \"all_metrics_json\": json.dumps(run[\"all_metrics\"]),\n",
    "            \"params_json\": json.dumps(run[\"params\"]),\n",
    "            \"model_uri\": run[\"model_uri\"]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    spark_df = spark.createDataFrame(df, schema=get_evaluation_table_schema())\n",
    "\n",
    "    try:\n",
    "        # Append only new records\n",
    "        spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "        print(f\"‚úÖ Successfully logged {len(run_list)} new entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to log results: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5Ô∏è‚É£ Display Summary\n",
    "# -------------------------\n",
    "\n",
    "def show_summary(run_list):\n",
    "    if not run_list:\n",
    "        print(\"\\nüìä No new runs to display\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüìä NEWLY LOGGED MODEL RESULTS:\\n\")\n",
    "    for rank, run in enumerate(run_list[:10], 1):\n",
    "        pm_val = run['primary_metric']\n",
    "        print(f\"{rank}. {run['run_name']} ‚Üí {config.PRIMARY_METRIC}: {pm_val:.4f if pm_val else 0.0}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6Ô∏è‚É£ Show All Unique Experiments\n",
    "# -------------------------\n",
    "\n",
    "def show_all_experiments():\n",
    "    \"\"\"Display all unique experiments in the evaluation table\"\"\"\n",
    "    print(\"\\nüìã ALL UNIQUE EXPERIMENTS IN EVALUATION TABLE:\\n\")\n",
    "    \n",
    "    try:\n",
    "        all_runs = spark.sql(f\"\"\"\n",
    "            SELECT DISTINCT run_name, primary_metric_value, timestamp\n",
    "            FROM {config.EVALUATION_LOG_TABLE}\n",
    "            ORDER BY primary_metric_value DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        results = all_runs.collect()\n",
    "        \n",
    "        for idx, row in enumerate(results, 1):\n",
    "            print(f\"{idx}. {row.run_name} ‚Üí {config.PRIMARY_METRIC}: {row.primary_metric_value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìä Total Unique Experiments: {len(results)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not fetch experiments: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# üöÄ MAIN EXECUTION\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    # Ensure table exists\n",
    "    ensure_table_exists()\n",
    "    \n",
    "    # Fetch new runs (excluding already logged)\n",
    "    run_list = get_recent_runs()\n",
    "    \n",
    "    # Log new results\n",
    "    log_results(run_list)\n",
    "    \n",
    "    # Show summary of newly logged\n",
    "    show_summary(run_list)\n",
    "    \n",
    "    # Show all unique experiments\n",
    "    show_all_experiments()\n",
    "\n",
    "    print(\"\\nüéâ Evaluation Completed Successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
