{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL REGISTRATION SCRIPT - MULTI MODEL (FIXED)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, BooleanType\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL REGISTRATION SYSTEM - MULTI MODEL + AUTOMATED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD CONFIG FILES ----------------------\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    with open(\"experiments_config.yml\", \"r\") as f:\n",
    "        experiments_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"‚úÖ Configuration files loaded\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# ‚úÖ ADD WIDGET LOGIC\n",
    "try:\n",
    "    dbutils.widgets.text(\"MODELS_TO_TRAIN\", \"\", \"Models to Register\")\n",
    "    dbutils.widgets.text(\"environment\", \"development\", \"Environment\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ‚úÖ MODEL SHORT NAME MAPPING (same as training script)\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"random_forest\": \"RF\",\n",
    "    \"xgboost\": \"XGB\",\n",
    "    \"logistic_regression\": \"LR\",\n",
    "    \"gradient_boosting\": \"GB\",\n",
    "    \"decision_tree\": \"DT\",\n",
    "    \"svm\": \"SVM\",\n",
    "    \"naive_bayes\": \"NB\",\n",
    "    \"knn\": \"KNN\"\n",
    "}\n",
    "\n",
    "def get_model_short_name(model_type):\n",
    "    \"\"\"\n",
    "    ‚úÖ Dynamically generate short name if not in mapping\n",
    "    Same logic as training script\n",
    "    \"\"\"\n",
    "    if model_type in MODEL_SHORT_NAMES:\n",
    "        return MODEL_SHORT_NAMES[model_type]\n",
    "    else:\n",
    "        words = model_type.split(\"_\")\n",
    "        return \"\".join([w[0].upper() for w in words if w])\n",
    "\n",
    "\n",
    "def get_models_to_register():\n",
    "    \"\"\"\n",
    "    Get models to register from widget or environment variable\n",
    "    Matches the logic from training script\n",
    "    \"\"\"\n",
    "    available_models = list(experiments_cfg.get(\"models\", {}).keys())\n",
    "    \n",
    "    if not available_models:\n",
    "        raise ValueError(\"‚ùå No models defined in experiments_config.yml\")\n",
    "    \n",
    "    value = None\n",
    "    try:\n",
    "        value = dbutils.widgets.get(\"MODELS_TO_TRAIN\")\n",
    "        print(f\"üìå MODELS_TO_TRAIN from Widget: '{value}'\")\n",
    "    except:\n",
    "        value = os.getenv(\"MODELS_TO_TRAIN\", \"\")\n",
    "        print(f\"üìå MODELS_TO_TRAIN from ENV: '{value}'\")\n",
    "    \n",
    "    if value:\n",
    "        value = value.strip()\n",
    "    \n",
    "    if not value or value == \"\" or value.lower() in [\"none\", \"null\", \"undefined\"]:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå MODELS_TO_TRAIN is not set!\\n\"\n",
    "            f\"   Available models: {available_models}\\n\"\n",
    "            f\"   Current value: '{value}'\"\n",
    "        )\n",
    "    \n",
    "    if value.lower() == \"all\":\n",
    "        print(f\"‚úÖ Registering ALL models: {available_models}\")\n",
    "        return available_models\n",
    "    \n",
    "    models = [m.strip() for m in value.split(\",\") if m.strip()]\n",
    "    \n",
    "    if not models:\n",
    "        raise ValueError(f\"‚ùå No valid models found in MODELS_TO_TRAIN='{value}'\")\n",
    "    \n",
    "    invalid_models = [m for m in models if m not in available_models]\n",
    "    \n",
    "    if invalid_models:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Invalid model names: {invalid_models}\\n\"\n",
    "            f\"   Available: {available_models}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Models to register: {models}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "# ---------------------- CONFIGURATION CLASS ----------------------\n",
    "class Config:\n",
    "    def __init__(self, model_type: str):\n",
    "        \"\"\"\n",
    "        Initialize config for a specific model type\n",
    "        Args:\n",
    "            model_type: e.g., 'random_forest', 'xgboost'\n",
    "        \"\"\"\n",
    "        UC_CATALOG = pipeline_cfg[\"models\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"models\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"models\"][\"base_name\"]\n",
    "        NAMING_FMT = pipeline_cfg[\"models\"][\"naming\"][\"format\"]\n",
    "\n",
    "        self.MODEL_NAME = NAMING_FMT.format(\n",
    "            catalog=UC_CATALOG,\n",
    "            schema=UC_SCHEMA,\n",
    "            base_name=BASE_NAME,\n",
    "            model_type=model_type\n",
    "        )\n",
    "        \n",
    "        self.MODEL_TYPE = model_type\n",
    "        \n",
    "        # ‚úÖ CRITICAL FIX: Use model-specific experiment name\n",
    "        BASE_EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        model_short = get_model_short_name(model_type)\n",
    "        self.EXPERIMENT_NAME = f\"{BASE_EXPERIMENT_NAME}_{model_short}\"\n",
    "        \n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "\n",
    "        self.REGISTRATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"registration_log\"]\n",
    "\n",
    "        self.SLACK_WEBHOOK = None\n",
    "        try:\n",
    "            self.SLACK_WEBHOOK = dbutils.secrets.get(\"shared-scope\", \"SLACK_WEBHOOK_URL\")\n",
    "            print(f\"   üîê Slack webhook loaded for {model_type}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ---------------------- SLACK NOTIFIER ----------------------\n",
    "class SlackNotifier:\n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "\n",
    "    def send(self, message: str, level: str = \"info\"):\n",
    "        if not self.webhook_url:\n",
    "            return\n",
    "        emoji = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warning\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\"}.get(level, \"‚ÑπÔ∏è\")\n",
    "        payload = {\"text\": f\"{emoji} {message}\"}\n",
    "\n",
    "        try:\n",
    "            requests.post(self.webhook_url, json=payload, timeout=5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ---------------------- INIT SPARK + MLFLOW ----------------------\n",
    "spark = SparkSession.builder.appName(\"ModelRegistrationMultiModel\").getOrCreate()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "# ---------------------- TABLE SCHEMA ----------------------\n",
    "def get_table_schema():\n",
    "    \"\"\"Define fixed schema for registration log table\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_name\", StringType(), True),\n",
    "        StructField(\"model_type\", StringType(), True),\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"primary_metric\", StringType(), True),\n",
    "        StructField(\"primary_metric_value\", DoubleType(), True),\n",
    "        StructField(\"metrics_json\", StringType(), True),\n",
    "        StructField(\"params_json\", StringType(), True),\n",
    "        StructField(\"registered\", BooleanType(), True),\n",
    "        StructField(\"registered_version\", StringType(), True),\n",
    "        StructField(\"reason\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "# ---------------------- TABLE CREATION ----------------------\n",
    "def ensure_table_exists(table_name: str):\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "        print(f\"   ‚úÖ Table exists: {table_name}\")\n",
    "    except:\n",
    "        print(f\"   üÜï Creating Delta table: {table_name}\")\n",
    "        schema = get_table_schema()\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "        print(f\"   ‚úÖ Table created: {table_name}\")\n",
    "\n",
    "\n",
    "# ---------------------- FETCH RUNS FOR MODEL TYPE ----------------------\n",
    "def get_runs_for_model(config: Config) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ‚úÖ FIXED: Fetch runs from model-specific experiment\n",
    "    \"\"\"\n",
    "    print(f\"   üìç Searching experiment: {config.EXPERIMENT_NAME}\")\n",
    "    \n",
    "    # ‚úÖ Get the correct experiment for this model type\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "        if experiment is None:\n",
    "            print(f\"   ‚ö†Ô∏è  Experiment not found: {config.EXPERIMENT_NAME}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error getting experiment: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"   üî¨ Experiment ID: {experiment.experiment_id}\")\n",
    "    \n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Filter by model type (extra safety check)\n",
    "    filtered_runs = [\n",
    "        run for run in runs\n",
    "        if config.MODEL_TYPE in (run.info.run_name or \"\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"   üîç Found {len(filtered_runs)} runs for {config.MODEL_TYPE}\")\n",
    "\n",
    "    return [{\n",
    "        \"run_id\": run.info.run_id,\n",
    "        \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "        \"metrics\": {m: run.data.metrics.get(m) for m in config.METRICS_TO_COMPARE if m in run.data.metrics},\n",
    "        \"params\": run.data.params,\n",
    "        \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "        \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\"\n",
    "    } for run in filtered_runs]\n",
    "\n",
    "\n",
    "# ---------------------- DUPLICATE CHECK ----------------------\n",
    "def is_duplicate_model(new_model: Dict, config: Config) -> bool:\n",
    "    \"\"\"\n",
    "    ‚úÖ ENHANCED: Check if model with similar metrics and params already exists\n",
    "    \n",
    "    Compares:\n",
    "    1. ALL metrics from config.METRICS_TO_COMPARE\n",
    "    2. ALL parameters\n",
    "    3. Uses tolerance threshold from config\n",
    "    \n",
    "    Returns True if duplicate found (skip registration)\n",
    "    \"\"\"\n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        print(f\"      ‚ÑπÔ∏è  Duplicate detection: DISABLED\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "        \n",
    "        if not versions:\n",
    "            print(f\"      ‚ÑπÔ∏è  No existing versions found - will register\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è  Could not fetch versions: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"      üîç Checking against {len(versions)} existing version(s)...\")\n",
    "\n",
    "    for version in versions:\n",
    "        try:\n",
    "            # Get the run details for this registered version\n",
    "            run = client.get_run(version.run_id)\n",
    "            \n",
    "            # ‚úÖ Check ALL metrics from config\n",
    "            metrics_differences = {}\n",
    "            all_metrics_match = True\n",
    "            \n",
    "            for metric_name in config.METRICS_TO_COMPARE:\n",
    "                existing_value = run.data.metrics.get(metric_name, 0)\n",
    "                new_value = new_model[\"metrics\"].get(metric_name, 0)\n",
    "                difference = abs(existing_value - new_value)\n",
    "                \n",
    "                metrics_differences[metric_name] = {\n",
    "                    \"existing\": existing_value,\n",
    "                    \"new\": new_value,\n",
    "                    \"diff\": difference,\n",
    "                    \"within_tolerance\": difference <= config.TOLERANCE\n",
    "                }\n",
    "                \n",
    "                if difference > config.TOLERANCE:\n",
    "                    all_metrics_match = False\n",
    "            \n",
    "            # ‚úÖ Check ALL parameters\n",
    "            params_match = run.data.params == new_model[\"params\"]\n",
    "            \n",
    "            # ‚úÖ If BOTH metrics and params match ‚Üí Duplicate!\n",
    "            if all_metrics_match and params_match:\n",
    "                print(f\"      ‚ö†Ô∏è  DUPLICATE DETECTED ‚Üí Matches existing v{version.version}\")\n",
    "                print(f\"         Metrics comparison (tolerance: {config.TOLERANCE}):\")\n",
    "                for metric_name, diff_info in metrics_differences.items():\n",
    "                    status = \"‚úì\" if diff_info[\"within_tolerance\"] else \"‚úó\"\n",
    "                    print(f\"           {status} {metric_name}: {diff_info['existing']:.4f} vs {diff_info['new']:.4f} (diff: {diff_info['diff']:.4f})\")\n",
    "                print(f\"         ‚úì Parameters: Exact match\")\n",
    "                return True\n",
    "            \n",
    "            # Show why it's NOT a duplicate (for debugging)\n",
    "            elif not all_metrics_match and not params_match:\n",
    "                print(f\"      ‚úì NOT duplicate of v{version.version} (metrics AND params differ)\")\n",
    "            elif not all_metrics_match:\n",
    "                print(f\"      ‚úì NOT duplicate of v{version.version} (metrics differ beyond tolerance)\")\n",
    "            elif not params_match:\n",
    "                print(f\"      ‚úì NOT duplicate of v{version.version} (parameters differ)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è  Error checking v{version.version}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"      ‚úÖ No duplicates found - will register as new version\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# ---------------------- CHECK IF ALREADY LOGGED ----------------------\n",
    "def is_already_logged(run_id: str, table_name: str) -> bool:\n",
    "    \"\"\"Check if this run_id is already in the registration log\"\"\"\n",
    "    try:\n",
    "        existing = spark.sql(f\"\"\"\n",
    "            SELECT run_id \n",
    "            FROM {table_name} \n",
    "            WHERE run_id = '{run_id}'\n",
    "            LIMIT 1\n",
    "        \"\"\").count()\n",
    "\n",
    "        return existing > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ---------------------- REGISTER MODEL ----------------------\n",
    "def register_model(model: Dict, config: Config):\n",
    "    \"\"\"\n",
    "    Register model to Unity Catalog Model Registry\n",
    "    Note: Duplicate check is done BEFORE calling this function\n",
    "    \"\"\"\n",
    "    print(f\"      üîÑ Registering to: {config.MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        reg = mlflow.register_model(model[\"model_uri\"], config.MODEL_NAME)\n",
    "        version = reg.version\n",
    "        print(f\"      ‚úÖ Registered as version: {version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Registration failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"run_id\", model[\"run_id\"])\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"run_name\", model[\"run_name\"])\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"model_type\", config.MODEL_TYPE)\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"primary_metric\", config.PRIMARY_METRIC)\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"primary_metric_value\", str(round(model[\"primary_metric\"], 4)))\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"registered_timestamp\", datetime.now().isoformat())\n",
    "        \n",
    "        # ‚úÖ Add all compared metrics as tags\n",
    "        for metric_name in config.METRICS_TO_COMPARE:\n",
    "            if metric_name in model[\"metrics\"]:\n",
    "                metric_value = model[\"metrics\"][metric_name]\n",
    "                client.set_model_version_tag(\n",
    "                    config.MODEL_NAME, \n",
    "                    version, \n",
    "                    f\"metric_{metric_name}\", \n",
    "                    str(round(metric_value, 4))\n",
    "                )\n",
    "        \n",
    "        print(f\"      ‚úÖ Tags added successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è  Failed to set tags: {e}\")\n",
    "\n",
    "    return version\n",
    "\n",
    "\n",
    "# ---------------------- LOG DECISION ----------------------\n",
    "def log_decision(model: Dict, config: Config, registered: bool, version: Optional[int], reason: str):\n",
    "    \"\"\"Log registration decision to Delta table\"\"\"\n",
    "\n",
    "    if is_already_logged(model[\"run_id\"], config.REGISTRATION_LOG_TABLE):\n",
    "        print(f\"      ‚ÑπÔ∏è  Already logged in table, skipping duplicate entry\")\n",
    "        return\n",
    "\n",
    "    version_str = str(version) if version is not None else \"N/A\"\n",
    "\n",
    "    record = {\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"run_id\": model[\"run_id\"],\n",
    "        \"run_name\": model[\"run_name\"],\n",
    "        \"model_type\": config.MODEL_TYPE,\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"primary_metric\": config.PRIMARY_METRIC,\n",
    "        \"primary_metric_value\": float(model[\"primary_metric\"]) if model[\"primary_metric\"] else 0.0,\n",
    "        \"metrics_json\": json.dumps(model[\"metrics\"]),\n",
    "        \"params_json\": json.dumps(model[\"params\"]),\n",
    "        \"registered\": registered,\n",
    "        \"registered_version\": version_str,\n",
    "        \"reason\": reason\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([record])\n",
    "    spark_df = spark.createDataFrame(df, schema=get_table_schema())\n",
    "\n",
    "    try:\n",
    "        # ‚úÖ CRITICAL FIX: Add mergeSchema option\n",
    "        spark_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(config.REGISTRATION_LOG_TABLE)\n",
    "        print(f\"      üìÑ Logged to: {config.REGISTRATION_LOG_TABLE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è  Failed to log: {e}\")\n",
    "\n",
    "\n",
    "# ---------------------- PROCESS SINGLE MODEL TYPE ----------------------\n",
    "def process_model_type(model_type: str, slack: SlackNotifier) -> Dict:\n",
    "    \"\"\"\n",
    "    Process registration for a single model type\n",
    "    Returns: dict with counts\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ PROCESSING MODEL TYPE: {model_type.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    config = Config(model_type)\n",
    "    \n",
    "    print(f\"üì¶ Model Registry Name: {config.MODEL_NAME}\")\n",
    "    print(f\"üî¨ Experiment Name: {config.EXPERIMENT_NAME}\")\n",
    "    print(f\"üîç Primary Metric: {config.PRIMARY_METRIC}\")\n",
    "    print(f\"üõ°Ô∏è  Duplicate Check: {'ENABLED' if config.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\\n\")\n",
    "    \n",
    "    ensure_table_exists(config.REGISTRATION_LOG_TABLE)\n",
    "    \n",
    "    runs = get_runs_for_model(config)\n",
    "    \n",
    "    if not runs:\n",
    "        print(f\"   ‚ö†Ô∏è  No runs found for {model_type}\")\n",
    "        return {\"registered\": 0, \"skipped\": 0, \"total\": 0}\n",
    "    \n",
    "    registered_count = 0\n",
    "    skipped_count = 0\n",
    "    processed_run_ids = set()\n",
    "    \n",
    "    for idx, model in enumerate(runs, start=1):\n",
    "        \n",
    "        if model['run_id'] in processed_run_ids:\n",
    "            continue\n",
    "        \n",
    "        processed_run_ids.add(model['run_id'])\n",
    "        \n",
    "        print(f\"\\n   [{idx}/{len(runs)}] Processing: {model['run_name']}\")\n",
    "        print(f\"      Run ID: {model['run_id']}\")\n",
    "        print(f\"      Primary Metric ({config.PRIMARY_METRIC}): {model['primary_metric']:.4f}\")\n",
    "        \n",
    "        # ‚úÖ Show all metrics being compared\n",
    "        print(f\"      Metrics to compare:\")\n",
    "        for metric_name in config.METRICS_TO_COMPARE:\n",
    "            metric_value = model[\"metrics\"].get(metric_name, 0)\n",
    "            print(f\"         ‚Ä¢ {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        if is_already_logged(model['run_id'], config.REGISTRATION_LOG_TABLE):\n",
    "            print(f\"      ‚è≠Ô∏è  Skipped ‚Äî Already logged in registration table\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # ‚úÖ CRITICAL: This checks for duplicates in Registry\n",
    "        if is_duplicate_model(model, config):\n",
    "            log_decision(model, config, False, None, \"‚ö† Duplicate metrics+params - Skipped\")\n",
    "            slack.send(f\"‚ö†Ô∏è Duplicate skipped: {model['run_name']}\", \"warning\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # ‚úÖ Not a duplicate, proceed with registration\n",
    "        version = register_model(model, config)\n",
    "        \n",
    "        if version:\n",
    "            log_decision(model, config, True, version, \"‚úî Registered successfully\")\n",
    "            slack.send(f\"‚úÖ Registered: {config.MODEL_NAME} v{version} ({model['run_name']})\", \"success\")\n",
    "            registered_count += 1\n",
    "        else:\n",
    "            # This should not happen now, but keep as fallback\n",
    "            log_decision(model, config, False, None, \"‚ö† Registration failed\")\n",
    "            slack.send(f\"‚ö†Ô∏è Registration failed: {model['run_name']}\", \"warning\")\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ {model_type.upper()} REGISTRATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   ‚úÖ Registered: {registered_count}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Skipped: {skipped_count}\")\n",
    "    print(f\"   üìä Total: {len(runs)}\")\n",
    "    \n",
    "    return {\n",
    "        \"registered\": registered_count,\n",
    "        \"skipped\": skipped_count,\n",
    "        \"total\": len(runs)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------- MAIN ----------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main registration pipeline:\n",
    "    1. Parse model types from widget/environment\n",
    "    2. For each model type, process all runs from correct experiment\n",
    "    3. Register unique models to Unity Catalog\n",
    "    \"\"\"\n",
    "    print(\"\\nüöÄ Starting Multi-Model Registration Pipeline...\\n\")\n",
    "    \n",
    "    try:\n",
    "        MODEL_TYPES = get_models_to_register()\n",
    "        print(f\"\\nüìã Models to register: {MODEL_TYPES}\\n\")\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        dbutils.notebook.exit(\"FAILED: Invalid MODELS_TO_TRAIN configuration\")\n",
    "        return\n",
    "    \n",
    "    slack = SlackNotifier(None)\n",
    "    \n",
    "    total_stats = {\n",
    "        \"registered\": 0,\n",
    "        \"skipped\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "    \n",
    "    results_by_model = {}\n",
    "    \n",
    "    for model_type in MODEL_TYPES:\n",
    "        try:\n",
    "            stats = process_model_type(model_type, slack)\n",
    "            results_by_model[model_type] = stats\n",
    "            \n",
    "            total_stats[\"registered\"] += stats[\"registered\"]\n",
    "            total_stats[\"skipped\"] += stats[\"skipped\"]\n",
    "            total_stats[\"total\"] += stats[\"total\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error processing {model_type}: {e}\")\n",
    "            results_by_model[model_type] = {\"error\": str(e)}\n",
    "            continue\n",
    "    \n",
    "    # ---------------------- FINAL SUMMARY ----------------------\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ ALL MODELS REGISTRATION COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä Summary by Model Type:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_type, stats in results_by_model.items():\n",
    "        if \"error\" in stats:\n",
    "            print(f\"   ‚ùå {model_type}: {stats['error']}\")\n",
    "        else:\n",
    "            print(f\"   {model_type}:\")\n",
    "            print(f\"      ‚úÖ Registered: {stats['registered']}\")\n",
    "            print(f\"      ‚ö†Ô∏è  Skipped: {stats['skipped']}\")\n",
    "            print(f\"      üìä Total: {stats['total']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà Overall Statistics:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   ‚úÖ Total Registered: {total_stats['registered']}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Total Skipped: {total_stats['skipped']}\")\n",
    "    print(f\"   üìä Total Processed: {total_stats['total']}\")\n",
    "    print(f\"   üì¶ Registration Log: {pipeline_cfg['tables']['registration_log']}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
