{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ MODEL REGISTRATION SCRIPT \n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ MODEL REGISTRATION SYSTEM (CONTROLLED MULTI-RUN REGISTRATION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD PIPELINE CONFIG ----------------------\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"âœ… pipeline_config.yml loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        \n",
    "        # Duplicate check settings\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "\n",
    "        self.EVALUATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "        \n",
    "        # â­ NEW TABLE FOR REGISTRATION HISTORY â­\n",
    "        self.REGISTRATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"registration_log\"]\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"\\nðŸ“Œ Model Registry: {config.MODEL_NAME}\")\n",
    "print(f\"ðŸ“Œ Duplicate Logic: {'ENABLED' if config.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- INIT MLFLOW + SPARK ----------------------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ModelRegistration\").getOrCreate()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    raise Exception(f\"Experiment '{config.EXPERIMENT_NAME}' not found!\")\n",
    "\n",
    "\n",
    "# ---------------------- FETCH ALL RUNS ----------------------\n",
    "\n",
    "def get_all_runs() -> List[Dict]:\n",
    "    print(\"\\nðŸ“ Fetching ALL experiment runs...\")\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "            \"metrics\": {m: run.data.metrics.get(m) for m in config.METRICS_TO_COMPARE},\n",
    "            \"params\": run.data.params,\n",
    "            \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\"\n",
    "        }\n",
    "        for run in runs\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------- DUPLICATE DETECTION ----------------------\n",
    "\n",
    "def is_duplicate_model(new_model: Dict) -> bool:\n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        return False\n",
    "\n",
    "    versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "\n",
    "    for version in versions:\n",
    "        run = client.get_run(version.run_id)\n",
    "\n",
    "        metric_match = all(\n",
    "            abs(run.data.metrics.get(m) - new_model[\"metrics\"].get(m)) <= config.TOLERANCE\n",
    "            for m in config.METRICS_TO_COMPARE\n",
    "            if run.data.metrics.get(m) is not None and new_model[\"metrics\"].get(m) is not None\n",
    "        )\n",
    "\n",
    "        params_match = (new_model[\"params\"] == run.data.params)\n",
    "\n",
    "        if metric_match and params_match:\n",
    "            print(f\"âš  Duplicate detected â†’ Matches version v{version.version}\")\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# ---------------------- REGISTER MODEL ----------------------\n",
    "\n",
    "def register_model(model: Dict):\n",
    "    if is_duplicate_model(model):\n",
    "        print(\"â›” SKIPPED â€” Duplicate Model\")\n",
    "        return None\n",
    "\n",
    "    new_version = mlflow.register_model(model[\"model_uri\"], config.MODEL_NAME)\n",
    "    print(f\"ðŸŽ¯ Registered version: {new_version.version}\")\n",
    "    return new_version.version\n",
    "\n",
    "\n",
    "# ---------------------- â­ NEW LOG TABLE WRITE â­ ----------------------\n",
    "\n",
    "def log_registration(model, registered, version, reason):\n",
    "    df = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"run_id\": model[\"run_id\"],\n",
    "        \"run_name\": model[\"run_name\"],\n",
    "        \"primary_metric_value\": model[\"primary_metric\"],\n",
    "        \"metrics_json\": json.dumps(model[\"metrics\"]),\n",
    "        \"params_json\": json.dumps(model[\"params\"]),\n",
    "        \"registered\": registered,\n",
    "        \"registered_version\": version,\n",
    "        \"reason\": reason\n",
    "    }])\n",
    "\n",
    "    spark.createDataFrame(df)\\\n",
    "        .write\\\n",
    "        .format(\"delta\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .saveAsTable(config.REGISTRATION_LOG_TABLE)\n",
    "\n",
    "    print(f\"ðŸ“„ Logged in: {config.REGISTRATION_LOG_TABLE}\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION ----------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\nðŸš€ Scanning all model runs...\")\n",
    "    runs = get_all_runs()\n",
    "\n",
    "    for idx, model in enumerate(runs, start=1):\n",
    "        print(f\"\\n---------- [{idx}/{len(runs)}] Processing: {model['run_name']} ({model['primary_metric']}) ----------\")\n",
    "\n",
    "        version = register_model(model)\n",
    "\n",
    "        if version:\n",
    "            log_registration(model, True, version, \"âœ” Registered (unique)\")\n",
    "        else:\n",
    "            log_registration(model, False, None, \"âš  Duplicate - Skipped\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Registration Complete â€” All Runs Processed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
