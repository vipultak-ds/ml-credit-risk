{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL REGISTRATION SCRIPT - NEW WORKFLOW (FIXED)\n",
    "# Purpose: Register best models with duplicate detection\n",
    "# Compatible with: train.py ‚Üí evaluation.py ‚Üí THIS SCRIPT\n",
    "# Config-driven from pipeline_config.yml\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import traceback\n",
    "import requests\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL REGISTRATION SYSTEM (NEW WORKFLOW)\")\n",
    "print(\"=\" * 80)\n",
    " \n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION\n",
    " \n",
    "print(\"\\nüìã Step 1: Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Configuration loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    print(\"üí° Please ensure pipeline_config.yml is in the notebook directory\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    " \n",
    "# ‚úÖ EXTRACT CONFIGURATION VALUES\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration manager - reads from pipeline_config.yml\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model configuration\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "        \n",
    "        # Build full model name\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.MODEL_TYPE = MODEL_TYPE\n",
    "        \n",
    "        # Experiment tracking\n",
    "        self.EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "        \n",
    "        # Metrics configuration\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "        \n",
    "        # Registry settings\n",
    "        self.REGISTRY_MODE = pipeline_cfg[\"registry\"][\"mode\"]\n",
    "        \n",
    "        # Aliases\n",
    "        self.STAGING_ALIAS = pipeline_cfg[\"aliases\"][\"staging\"]\n",
    "        self.PRODUCTION_ALIAS = pipeline_cfg[\"aliases\"][\"production\"]\n",
    "        self.BEST_ALIAS = pipeline_cfg[\"aliases\"][\"best\"]\n",
    "        \n",
    "        # Tables\n",
    "        self.EVALUATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "        \n",
    "        # Slack notification settings\n",
    "        self.SLACK_ENABLED = pipeline_cfg[\"notifications\"][\"enabled\"]\n",
    "        self.SLACK_WEBHOOK_URL = self._get_slack_webhook()\n",
    "        \n",
    "        print(f\"\\nüìä Configuration Summary:\")\n",
    "        print(f\"   Model Type: {self.MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Model Name: {self.MODEL_NAME}\")\n",
    "        print(f\"   Experiment: {self.EXPERIMENT_NAME}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC}\")\n",
    "        print(f\"   Duplicate Detection: {'ENABLED' if self.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\")\n",
    "        print(f\"   Tolerance: {self.TOLERANCE}\")\n",
    "        print(f\"   Registry Mode: {self.REGISTRY_MODE}\")\n",
    "        print(f\"   Slack Notifications: {'ENABLED' if self.SLACK_WEBHOOK_URL else 'DISABLED'}\")\n",
    "    \n",
    "    def _get_slack_webhook(self) -> Optional[str]:\n",
    "        \"\"\"Safely retrieve Slack webhook URL from Databricks secrets\"\"\"\n",
    "        if not self.SLACK_ENABLED:\n",
    "            return None\n",
    "        \n",
    "        # Try to get from Databricks secrets\n",
    "        try:\n",
    "            # Try common secret scopes\n",
    "            scopes = [\"shared-scope\", \"dev-scope\", \"prod-scope\", \"ml-scope\"]\n",
    "            for scope in scopes:\n",
    "                try:\n",
    "                    webhook = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "                    if webhook and webhook.strip():\n",
    "                        print(f\"   ‚úÖ Slack webhook found in scope '{scope}'\")\n",
    "                        return webhook\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            print(\"   ‚ÑπÔ∏è  No Slack webhook found in secrets\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not access secrets: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize config\n",
    "config = Config()\n",
    "\n",
    "print(\"=\" * 80)\n",
    " \n",
    "# üì¢ SLACK NOTIFICATION HELPER\n",
    "\n",
    "class SlackNotifier:\n",
    "    \"\"\"Enhanced Slack notification handler\"\"\"\n",
    "    \n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "        self.enabled = webhook_url is not None and webhook_url.strip() != \"\"\n",
    "        \n",
    "    def send(self, message: str, level: str = \"info\", extra_fields: Optional[Dict] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Send Slack notification with error handling\n",
    "        \n",
    "        Args:\n",
    "            message: Main message text\n",
    "            level: Message level (info, success, warning, error)\n",
    "            extra_fields: Additional fields to include in message\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if sent successfully, False otherwise\n",
    "        \"\"\"\n",
    "        if not self.enabled:\n",
    "            print(f\"üì¢ [SLACK DISABLED] {message}\")\n",
    "            return False\n",
    "        \n",
    "        # Emoji mapping\n",
    "        emoji_map = {\n",
    "            \"info\": \"‚ÑπÔ∏è\",\n",
    "            \"success\": \"‚úÖ\",\n",
    "            \"warning\": \"‚ö†Ô∏è\",\n",
    "            \"error\": \"‚ùå\",\n",
    "            \"trophy\": \"üèÜ\",\n",
    "            \"rocket\": \"üöÄ\"\n",
    "        }\n",
    "        \n",
    "        # Build message\n",
    "        formatted_message = f\"{emoji_map.get(level, '‚ÑπÔ∏è')} *{message}*\"\n",
    "        \n",
    "        # Add extra fields if provided\n",
    "        if extra_fields:\n",
    "            formatted_message += \"\\n\"\n",
    "            for key, value in extra_fields.items():\n",
    "                formatted_message += f\"\\n‚Ä¢ *{key}:* {value}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"text\": formatted_message,\n",
    "            \"username\": \"ML Pipeline Bot\",\n",
    "            \"icon_emoji\": \":robot_face:\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.webhook_url,\n",
    "                json=payload,\n",
    "                timeout=5\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"üì¢ Slack notification sent successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Slack error: {response.status_code} - {response.text}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Slack notification failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_registration_success(self, model_name: str, version: int, metrics: Dict) -> bool:\n",
    "        \"\"\"Send success notification for model registration\"\"\"\n",
    "        extra = {\n",
    "            \"Model\": model_name,\n",
    "            \"Version\": f\"v{version}\",\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        # Add key metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if metric_value is not None:\n",
    "                extra[metric_name] = f\"{metric_value:.4f}\"\n",
    "        \n",
    "        return self.send(\n",
    "            \"Model Registration Successful\",\n",
    "            level=\"success\",\n",
    "            extra_fields=extra\n",
    "        )\n",
    "    \n",
    "    def send_registration_skipped(self, model_name: str, reason: str) -> bool:\n",
    "        \"\"\"Send notification when registration is skipped\"\"\"\n",
    "        extra = {\n",
    "            \"Model\": model_name,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        return self.send(\n",
    "            \"Model Registration Skipped\",\n",
    "            level=\"warning\",\n",
    "            extra_fields=extra\n",
    "        )\n",
    "    \n",
    "    def send_error(self, error_message: str, details: Optional[str] = None) -> bool:\n",
    "        \"\"\"Send error notification\"\"\"\n",
    "        extra = {\n",
    "            \"Error\": error_message,\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        if details:\n",
    "            extra[\"Details\"] = details\n",
    "        \n",
    "        return self.send(\n",
    "            \"Registration Pipeline Error\",\n",
    "            level=\"error\",\n",
    "            extra_fields=extra\n",
    "        )\n",
    " \n",
    "# ‚úÖ INITIALIZE MLFLOW & SPARK\n",
    "\n",
    "print(\"\\nüîß Step 2: Initializing MLflow and Spark...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"ModelRegistration\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Get experiment\n",
    "    experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        raise Exception(f\"Experiment '{config.EXPERIMENT_NAME}' not found!\")\n",
    "    \n",
    "    print(\"‚úÖ MLflow and Spark initialized successfully\")\n",
    "    print(f\"   Experiment ID: {experiment.experiment_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize Slack notifier\n",
    "slack = SlackNotifier(config.SLACK_WEBHOOK_URL)\n",
    "\n",
    "# Send startup notification\n",
    "slack.send(\n",
    "    \"Model Registration Pipeline Started\",\n",
    "    level=\"info\",\n",
    "    extra_fields={\n",
    "        \"Model Type\": config.MODEL_TYPE.upper(),\n",
    "        \"Experiment\": config.EXPERIMENT_NAME\n",
    "    }\n",
    ")\n",
    " \n",
    "# üìã STEP 1: GET BEST RUN FROM LATEST TRAINING\n",
    "\n",
    "def get_best_run_from_training() -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Get the best run from the latest training session\n",
    "    Based on primary metric defined in config\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 1: Finding Best Run from Training\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Searching for best run by {config.PRIMARY_METRIC}...\")\n",
    "        \n",
    "        # Search runs ordered by primary metric\n",
    "        runs = client.search_runs(\n",
    "            [experiment.experiment_id],\n",
    "            order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "            max_results=1\n",
    "        )\n",
    "        \n",
    "        if not runs:\n",
    "            print(\"‚ùå No runs found in experiment!\")\n",
    "            return None\n",
    "        \n",
    "        best_run = runs[0]\n",
    "        \n",
    "        # Extract all metrics for comparison\n",
    "        all_metrics = {}\n",
    "        for metric in config.METRICS_TO_COMPARE:\n",
    "            all_metrics[metric] = best_run.data.metrics.get(metric)\n",
    "        \n",
    "        run_info = {\n",
    "            \"run_id\": best_run.info.run_id,\n",
    "            \"run_name\": best_run.info.run_name,\n",
    "            \"primary_metric\": best_run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"all_metrics\": all_metrics,\n",
    "            \"params\": best_run.data.params,\n",
    "            \"model_uri\": f\"runs:/{best_run.info.run_id}/{config.ARTIFACT_PATH}\",\n",
    "            \"timestamp\": datetime.fromtimestamp(best_run.info.start_time / 1000)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Best run found:\")\n",
    "        print(f\"   Run ID: {run_info['run_id']}\")\n",
    "        print(f\"   Run Name: {run_info['run_name']}\")\n",
    "        print(f\"   {config.PRIMARY_METRIC}: {run_info['primary_metric']:.4f}\")\n",
    "        print(f\"   Model URI: {run_info['model_uri']}\")\n",
    "        \n",
    "        print(f\"\\nüìä All Comparison Metrics:\")\n",
    "        for metric_name, metric_value in all_metrics.items():\n",
    "            if metric_value is not None:\n",
    "                print(f\"   {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        return run_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to get best run: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    " \n",
    "# üîç STEP 2: CHECK FOR DUPLICATE MODELS\n",
    "\n",
    "def is_duplicate_model(new_model: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if model with same metrics already exists in registry\n",
    "    Compares multiple metrics defined in config\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 2: Duplicate Detection\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        print(\"‚ÑπÔ∏è  Duplicate detection disabled in config\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üîé Checking for duplicates in registry...\")\n",
    "    print(f\"   Model: {config.MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "        versions_list = list(versions)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  Model not found in registry (first registration): {e}\")\n",
    "        return False\n",
    "    \n",
    "    if not versions_list:\n",
    "        print(\"‚ÑπÔ∏è  No existing versions found (first registration)\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üìä Found {len(versions_list)} existing version(s), checking metrics...\")\n",
    "    \n",
    "    for version in versions_list:\n",
    "        try:\n",
    "            # Get run details\n",
    "            run = client.get_run(version.run_id)\n",
    "            \n",
    "            # Compare all metrics defined in config\n",
    "            all_metrics_match = True\n",
    "            metric_comparisons = []\n",
    "            \n",
    "            for metric_name in config.METRICS_TO_COMPARE:\n",
    "                existing_value = run.data.metrics.get(metric_name)\n",
    "                new_value = new_model['all_metrics'].get(metric_name)\n",
    "                \n",
    "                if existing_value is None or new_value is None:\n",
    "                    continue\n",
    "                \n",
    "                diff = abs(existing_value - new_value)\n",
    "                matches = diff < config.TOLERANCE\n",
    "                \n",
    "                metric_comparisons.append({\n",
    "                    'metric': metric_name,\n",
    "                    'existing': existing_value,\n",
    "                    'new': new_value,\n",
    "                    'diff': diff,\n",
    "                    'matches': matches\n",
    "                })\n",
    "                \n",
    "                if not matches:\n",
    "                    all_metrics_match = False\n",
    "            \n",
    "            if all_metrics_match and metric_comparisons:\n",
    "                print(f\"\\n‚ö†Ô∏è  DUPLICATE FOUND!\")\n",
    "                print(f\"   Version: v{version.version}\")\n",
    "                print(f\"   Run ID: {version.run_id}\")\n",
    "                print(f\"\\n   Metric Comparison:\")\n",
    "                for comp in metric_comparisons:\n",
    "                    print(f\"      {comp['metric']}: diff={comp['diff']:.6f} (tolerance={config.TOLERANCE})\")\n",
    "                \n",
    "                # Send Slack notification about duplicate\n",
    "                slack.send(\n",
    "                    \"Duplicate Model Detected\",\n",
    "                    level=\"warning\",\n",
    "                    extra_fields={\n",
    "                        \"Model\": config.MODEL_NAME,\n",
    "                        \"Existing Version\": f\"v{version.version}\",\n",
    "                        \"Reason\": \"Same metrics within tolerance\",\n",
    "                        \"Tolerance\": str(config.TOLERANCE)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error checking version {version.version}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"‚úÖ No duplicate found - model is unique\")\n",
    "    return False\n",
    " \n",
    "# üöÄ STEP 3: REGISTER MODEL TO UNITY CATALOG\n",
    "\n",
    "def register_model(run_info: Dict) -> Optional[any]:\n",
    "    \"\"\"Register model to Unity Catalog\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 3: Model Registration\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if is_duplicate_model(run_info):\n",
    "        print(\"\\n‚ö†Ô∏è  Registration SKIPPED: Duplicate model detected\")\n",
    "        print(\"   Model with same metrics already exists in registry\")\n",
    "        \n",
    "        # Send Slack notification\n",
    "        slack.send_registration_skipped(\n",
    "            config.MODEL_NAME,\n",
    "            \"Duplicate model - same metrics already registered\"\n",
    "        )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n‚è≥ Registering model to Unity Catalog...\")\n",
    "        print(f\"   Model Name: {config.MODEL_NAME}\")\n",
    "        print(f\"   Source URI: {run_info['model_uri']}\")\n",
    "        \n",
    "        # Register model\n",
    "        new_version = mlflow.register_model(\n",
    "            run_info['model_uri'],\n",
    "            config.MODEL_NAME\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ MODEL REGISTERED SUCCESSFULLY!\")\n",
    "        print(f\"   Model: {config.MODEL_NAME}\")\n",
    "        print(f\"   Version: v{new_version.version}\")\n",
    "        print(f\"   Run ID: {run_info['run_id']}\")\n",
    "        \n",
    "        # Send Slack success notification\n",
    "        slack.send_registration_success(\n",
    "            config.MODEL_NAME,\n",
    "            new_version.version,\n",
    "            run_info['all_metrics']\n",
    "        )\n",
    "        \n",
    "        return new_version\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Registration failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Send Slack error notification\n",
    "        slack.send_error(\n",
    "            \"Model registration failed\",\n",
    "            details=str(e)\n",
    "        )\n",
    "        \n",
    "        return None\n",
    " \n",
    "# üè∑Ô∏è  STEP 4: ADD METADATA TAGS\n",
    "\n",
    "def add_metadata_tags(version_number: int, run_info: Dict) -> bool:\n",
    "    \"\"\"Add metadata tags to registered model version\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 4: Adding Metadata Tags\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        tags = {\n",
    "            \"model_type\": config.MODEL_TYPE,\n",
    "            \"registered_from\": \"new_registration_pipeline\",\n",
    "            \"registration_timestamp\": datetime.now().isoformat(),\n",
    "            \"source_run_id\": run_info['run_id'],\n",
    "            \"source_run_name\": run_info['run_name'],\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": f\"{run_info['primary_metric']:.6f}\",\n",
    "            \"artifact_path\": config.ARTIFACT_PATH,\n",
    "            \"training_timestamp\": run_info['timestamp'].isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add all comparison metrics as tags\n",
    "        for metric_name, metric_value in run_info['all_metrics'].items():\n",
    "            if metric_value is not None:\n",
    "                tags[f\"metric_{metric_name}\"] = f\"{metric_value:.6f}\"\n",
    "        \n",
    "        print(f\"   Adding {len(tags)} metadata tags...\")\n",
    "        \n",
    "        for key, value in tags.items():\n",
    "            try:\n",
    "                client.set_model_version_tag(\n",
    "                    config.MODEL_NAME,\n",
    "                    version_number,\n",
    "                    key,\n",
    "                    str(value)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Failed to set tag '{key}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   ‚úÖ Metadata tags added successfully\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to add tags: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    " \n",
    "# üìù STEP 5: LOG REGISTRATION DECISION\n",
    "\n",
    "def log_registration_decision(run_info: Dict, registered: bool, version: Optional[int], reason: str) -> None:\n",
    "    \"\"\"Log registration decision to Delta table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 5: Logging Decision\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        log_data = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"run_id\": run_info[\"run_id\"],\n",
    "            \"run_name\": run_info[\"run_name\"],\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"model_type\": config.MODEL_TYPE,\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": run_info[\"primary_metric\"],\n",
    "            \"all_metrics_json\": json.dumps(run_info[\"all_metrics\"]),\n",
    "            \"params_json\": json.dumps(run_info[\"params\"]),\n",
    "            \"model_uri\": run_info[\"model_uri\"],\n",
    "            \"registered\": registered,\n",
    "            \"registered_version\": version if version else None,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame([log_data])\n",
    "        \n",
    "        print(f\"   Logging to: {config.EVALUATION_LOG_TABLE}\")\n",
    "        spark.createDataFrame(df).write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "            config.EVALUATION_LOG_TABLE\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Decision logged successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to log decision: {e}\")\n",
    "        traceback.print_exc()\n",
    " \n",
    "# üìä STEP 6: DISPLAY SUMMARY\n",
    "\n",
    "def display_summary(run_info: Dict, version_number: Optional[int], registered: bool) -> None:\n",
    "    \"\"\"Display registration summary\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if registered:\n",
    "        print(\"‚úÖ‚úÖ MODEL REGISTRATION COMPLETE ‚úÖ‚úÖ\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è‚ö†Ô∏è  MODEL REGISTRATION SKIPPED ‚ö†Ô∏è‚ö†Ô∏è\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüìä Source Model:\")\n",
    "    print(f\"   Model Type: {config.MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Run ID: {run_info['run_id']}\")\n",
    "    print(f\"   Run Name: {run_info['run_name']}\")\n",
    "    print(f\"   {config.PRIMARY_METRIC}: {run_info['primary_metric']:.4f}\")\n",
    "    \n",
    "    if registered and version_number:\n",
    "        print(f\"\\nüèÜ Registered Model:\")\n",
    "        print(f\"   Registry: {config.MODEL_NAME}\")\n",
    "        print(f\"   Version: v{version_number}\")\n",
    "        \n",
    "        print(f\"\\nüìå Next Steps:\")\n",
    "        print(f\"   1. Verify model in Unity Catalog\")\n",
    "        print(f\"   2. Run UAT/validation tests\")\n",
    "        print(f\"   3. Promote to production if tests pass\")\n",
    "    else:\n",
    "        print(f\"\\n‚è≠Ô∏è  Registration skipped (duplicate detected)\")\n",
    "        print(f\"   Check existing versions in: {config.MODEL_NAME}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    " \n",
    "# üé¨ MAIN EXECUTION\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main registration pipeline\"\"\"\n",
    "    registered = False\n",
    "    version_number = None\n",
    "    reason = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get best run from training\n",
    "        run_info = get_best_run_from_training()\n",
    "        if not run_info:\n",
    "            print(\"\\n‚ùå No run found to register\")\n",
    "            reason = \"No run found in experiment\"\n",
    "            return\n",
    "        \n",
    "        # Step 2 & 3: Check duplicates and register\n",
    "        new_version = register_model(run_info)\n",
    "        \n",
    "        if new_version:\n",
    "            registered = True\n",
    "            version_number = new_version.version\n",
    "            reason = \"Successfully registered (unique model)\"\n",
    "            \n",
    "            # Step 4: Add metadata tags\n",
    "            add_metadata_tags(version_number, run_info)\n",
    "        else:\n",
    "            registered = False\n",
    "            reason = \"Duplicate model detected - skipped registration\"\n",
    "        \n",
    "        # Step 5: Log decision\n",
    "        log_registration_decision(run_info, registered, version_number, reason)\n",
    "        \n",
    "        # Step 6: Display summary\n",
    "        display_summary(run_info, version_number, registered)\n",
    "        \n",
    "        # Send final Slack summary\n",
    "        if registered and version_number:\n",
    "            slack.send(\n",
    "                \"Registration Pipeline Completed Successfully\",\n",
    "                level=\"trophy\",\n",
    "                extra_fields={\n",
    "                    \"Model\": config.MODEL_NAME,\n",
    "                    \"Version\": f\"v{version_number}\",\n",
    "                    \"Run Name\": run_info['run_name'],\n",
    "                    f\"{config.PRIMARY_METRIC}\": f\"{run_info['primary_metric']:.4f}\",\n",
    "                    \"Status\": \"Ready for UAT/Production\"\n",
    "                }\n",
    "            )\n",
    "        elif not registered:\n",
    "            slack.send(\n",
    "                \"Registration Pipeline Completed - No Registration\",\n",
    "                level=\"info\",\n",
    "                extra_fields={\n",
    "                    \"Model\": config.MODEL_NAME,\n",
    "                    \"Reason\": reason,\n",
    "                    \"Run Name\": run_info['run_name']\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Save task values for workflow\n",
    "        try:\n",
    "            dbutils.jobs.taskValues.set(key=\"model_type\", value=config.MODEL_TYPE)\n",
    "            dbutils.jobs.taskValues.set(key=\"model_name\", value=config.MODEL_NAME)\n",
    "            dbutils.jobs.taskValues.set(key=\"registered\", value=registered)\n",
    "            if version_number:\n",
    "                dbutils.jobs.taskValues.set(key=\"model_version\", value=version_number)\n",
    "            print(\"\\n‚úÖ Task values saved for workflow\")\n",
    "        except:\n",
    "            print(\"\\n‚ÑπÔ∏è  Not running in Databricks workflow - skipping task values\")\n",
    "        \n",
    "        print(\"\\nüéâ Registration pipeline completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Registration pipeline failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Send critical error notification\n",
    "        slack.send_error(\n",
    "            \"Registration Pipeline Failed\",\n",
    "            details=f\"{type(e).__name__}: {str(e)}\"\n",
    "        )\n",
    "        \n",
    "        # Try to log failure\n",
    "        try:\n",
    "            if run_info:\n",
    "                log_registration_decision(\n",
    "                    run_info,\n",
    "                    False,\n",
    "                    None,\n",
    "                    f\"Registration failed: {str(e)}\"\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        sys.exit(1)\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
