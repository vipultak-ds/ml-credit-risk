{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ MODEL REGISTRATION SCRIPT (WITH SLACK NOTIFICATIONS ADDED)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import traceback\n",
    "import requests\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from IPython import get_ipython\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ MODEL REGISTRATION SYSTEM (CONTROLLED MULTI-RUN + SLACK NOTIFICATIONS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD PIPELINE CONFIG ----------------------\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"âœ… pipeline_config.yml loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        \n",
    "        # Duplicate check settings\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "\n",
    "        self.EVALUATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "\n",
    "        # ---------------------- NEW: SLACK CONFIG ----------------------\n",
    "        # Try shared-scope then dev-scope (same behavior as V2)\n",
    "        self.SLACK_WEBHOOK = None\n",
    "        for scope in [\"shared-scope\", \"dev-scope\"]:\n",
    "            try:\n",
    "                self.SLACK_WEBHOOK = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "                print(f\"ðŸ” Slack webhook loaded from secret scope: {scope}\")\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if not self.SLACK_WEBHOOK:\n",
    "            print(\"âš  Slack webhook NOT configured. Notifications disabled.\")\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"\\nðŸ“Œ Model Registry: {config.MODEL_NAME}\")\n",
    "print(f\"ðŸ“Œ Duplicate Logic: {'ENABLED' if config.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\")\n",
    "print(f\"ðŸ“Œ Slack Enabled: {'YES' if config.SLACK_WEBHOOK else 'NO'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ---------------------- SLACK NOTIFIER (FROM V2) ----------------------\n",
    "\n",
    "class SlackNotifier:\n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "\n",
    "    def send(self, message: str, level: str = \"info\"):\n",
    "        if not self.webhook_url:\n",
    "            print(f\"ðŸ“¢ [Slack Disabled] {message}\")\n",
    "            return\n",
    "        \n",
    "        emoji = {\"info\":\"â„¹ï¸\",\"success\":\"âœ…\",\"warning\":\"âš ï¸\",\"error\":\"âŒ\"}.get(level,\"â„¹ï¸\")\n",
    "        payload = {\"text\": f\"{emoji} {message}\"}\n",
    "\n",
    "        try:\n",
    "            requests.post(self.webhook_url, json=payload, timeout=5)\n",
    "            print(f\"ðŸ“¨ Slack message sent: {level}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Slack send failed: {e}\")\n",
    "\n",
    "slack = SlackNotifier(config.SLACK_WEBHOOK)\n",
    "\n",
    "\n",
    "# ---------------------- INIT MLFLOW + SPARK ----------------------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ModelRegistration\").getOrCreate()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    raise Exception(f\"Experiment '{config.EXPERIMENT_NAME}' not found!\")\n",
    "\n",
    "\n",
    "# ---------------------- FETCH ALL RUNS ----------------------\n",
    "\n",
    "def get_all_runs() -> List[Dict]:\n",
    "    print(\"\\nðŸ“ Fetching ALL experiment runs...\")\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "            \"metrics\": {m: run.data.metrics.get(m) for m in config.METRICS_TO_COMPARE},\n",
    "            \"params\": run.data.params,\n",
    "            \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\"\n",
    "        }\n",
    "        for run in runs\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------- DUPLICATE DETECTION ----------------------\n",
    "\n",
    "def is_duplicate_model(new_model: Dict) -> bool:\n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        return False\n",
    "\n",
    "    versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "\n",
    "    for version in versions:\n",
    "        run = client.get_run(version.run_id)\n",
    "\n",
    "        metric_match = all(\n",
    "            abs(run.data.metrics.get(m) - new_model[\"metrics\"].get(m)) <= config.TOLERANCE\n",
    "            for m in config.METRICS_TO_COMPARE\n",
    "            if run.data.metrics.get(m) is not None and new_model[\"metrics\"].get(m) is not None\n",
    "        )\n",
    "\n",
    "        params_match = (new_model[\"params\"] == run.data.params)\n",
    "\n",
    "        if metric_match and params_match:\n",
    "            print(f\"âš  Duplicate detected â†’ Matches version v{version.version}\")\n",
    "            slack.send(f\"âš  Duplicate model skipped (Run: {new_model['run_name']})\", \"warning\")\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# ---------------------- REGISTER MODEL ----------------------\n",
    "\n",
    "def register_model(model: Dict):\n",
    "    if is_duplicate_model(model):\n",
    "        return None\n",
    "\n",
    "    new_version = mlflow.register_model(model[\"model_uri\"], config.MODEL_NAME)\n",
    "    print(f\"ðŸŽ¯ Registered version: {new_version.version}\")\n",
    "\n",
    "    slack.send(f\"âœ… Model registered: {config.MODEL_NAME} (v{new_version.version})\", \"success\")\n",
    "\n",
    "    return new_version.version\n",
    "\n",
    "\n",
    "# ---------------------- LOG DECISION ----------------------\n",
    "\n",
    "def log_decision(model, registered, version, reason):\n",
    "    df = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"run_id\": model[\"run_id\"],\n",
    "        \"run_name\": model[\"run_name\"],\n",
    "        \"primary_metric_value\": model[\"primary_metric\"],\n",
    "        \"metrics_json\": json.dumps(model[\"metrics\"]),\n",
    "        \"params_json\": json.dumps(model[\"params\"]),\n",
    "        \"registered\": registered,\n",
    "        \"registered_version\": version,\n",
    "        \"reason\": reason\n",
    "    }])\n",
    "\n",
    "    spark.createDataFrame(df).write.format(\"delta\").mode(\"append\").saveAsTable(config.EVALUATION_LOG_TABLE)\n",
    "    print(\"ðŸ“„ Logged in evaluation table\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION ----------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\nðŸš€ Scanning all model runs...\")\n",
    "    runs = get_all_runs()\n",
    "\n",
    "    for idx, model in enumerate(runs, start=1):\n",
    "        print(f\"\\n---------- [{idx}/{len(runs)}] Processing: {model['run_name']} ----------\")\n",
    "\n",
    "        version = register_model(model)\n",
    "\n",
    "        if version:\n",
    "            log_decision(model, True, version, \"âœ” Registered (unique)\")\n",
    "        else:\n",
    "            log_decision(model, False, None, \"âš  Duplicate - Skipped\")\n",
    "\n",
    "    slack.send(\"ðŸ“ Model registration pipeline completed.\", \"info\")\n",
    "    print(\"\\nðŸŽ‰ Registration Complete â€” All Runs Processed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
