{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL REGISTRATION SCRIPT - FIXED & CONFIG DRIVEN\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL REGISTRATION SYSTEM - AUTOMATED & DUPLICATE SAFE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD PIPELINE CONFIG ----------------------\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"‚úÖ pipeline_config.yml loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        \n",
    "        # Duplicate check settings\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "\n",
    "        self.REGISTRATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"registration_log\"]\n",
    "\n",
    "        # Slack config (optional)\n",
    "        self.SLACK_WEBHOOK = None\n",
    "        try:\n",
    "            self.SLACK_WEBHOOK = dbutils.secrets.get(\"shared-scope\", \"SLACK_WEBHOOK_URL\")\n",
    "            print(\"üîê Slack webhook loaded\")\n",
    "        except:\n",
    "            print(\"‚ö† Slack webhook NOT configured (Optional)\")\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"\\nüìå Model Registry: {config.MODEL_NAME}\")\n",
    "print(f\"üìå Duplicate Logic: {'ENABLED' if config.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\")\n",
    "print(f\"üìå Primary Metric: {config.PRIMARY_METRIC}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ---------------------- SLACK NOTIFIER (Optional) ----------------------\n",
    "\n",
    "class SlackNotifier:\n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "\n",
    "    def send(self, message: str, level: str = \"info\"):\n",
    "        if not self.webhook_url:\n",
    "            return\n",
    "        \n",
    "        emoji = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warning\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\"}.get(level,\"‚ÑπÔ∏è\")\n",
    "        payload = {\"text\": f\"{emoji} {message}\"}\n",
    "\n",
    "        try:\n",
    "            requests.post(self.webhook_url, json=payload, timeout=5)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Slack send failed: {e}\")\n",
    "\n",
    "slack = SlackNotifier(config.SLACK_WEBHOOK)\n",
    "\n",
    "\n",
    "# ---------------------- INIT MLFLOW + SPARK ----------------------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ModelRegistration\").getOrCreate()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    raise Exception(f\"Experiment '{config.EXPERIMENT_NAME}' not found!\")\n",
    "\n",
    "\n",
    "# ---------------------- AUTO TABLE CREATOR (NEW) ----------------------\n",
    "\n",
    "def ensure_table_exists(table_name: str, df_schema, spark):\n",
    "    \"\"\"Create delta table automatically if not exists.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "        print(f\"üìå Table exists: {table_name}\")\n",
    "    except Exception:\n",
    "        print(f\"üÜï Creating new Delta table: {table_name}\")\n",
    "        (\n",
    "            spark.createDataFrame([], df_schema)\n",
    "            .write.format(\"delta\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "        print(f\"‚úÖ Table created: {table_name}\")\n",
    "\n",
    "\n",
    "# ---------------------- FETCH ALL RUNS ----------------------\n",
    "\n",
    "def get_all_runs() -> List[Dict]:\n",
    "    print(\"\\nüìç Fetching ALL experiment runs...\")\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    processed_runs = []\n",
    "\n",
    "    for run in runs:\n",
    "        all_metrics = {m: run.data.metrics.get(m) for m in config.METRICS_TO_COMPARE if m in run.data.metrics}\n",
    "        model_uri = f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\"\n",
    "\n",
    "        processed_runs.append({\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "            \"metrics\": all_metrics,\n",
    "            \"params\": run.data.params,\n",
    "            \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "            \"model_uri\": model_uri\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Found {len(processed_runs)} runs in experiment\")\n",
    "    return processed_runs\n",
    "\n",
    "\n",
    "# ---------------------- DUPLICATE DETECTION ----------------------\n",
    "\n",
    "def is_duplicate_model(new_model: Dict) -> bool:\n",
    "    \"\"\"Check if model with same metrics and params already exists\"\"\"\n",
    "    \n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    for version in versions:\n",
    "        try:\n",
    "            run = client.get_run(version.run_id)\n",
    "\n",
    "            metric_match = True\n",
    "            for m in config.METRICS_TO_COMPARE:\n",
    "                old_val = run.data.metrics.get(m)\n",
    "                new_val = new_model[\"metrics\"].get(m)\n",
    "                \n",
    "                if old_val is not None and new_val is not None:\n",
    "                    if abs(old_val - new_val) > config.TOLERANCE:\n",
    "                        metric_match = False\n",
    "                        break\n",
    "\n",
    "            params_match = (new_model[\"params\"] == run.data.params)\n",
    "\n",
    "            if metric_match and params_match:\n",
    "                print(f\"   ‚ö† Duplicate detected ‚Üí Matches version v{version.version}\")\n",
    "                return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö† Error checking version {version.version}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# ---------------------- REGISTER MODEL ----------------------\n",
    "\n",
    "def register_model(model: Dict):\n",
    "    \"\"\"Register model and add all metadata as tags\"\"\"\n",
    "    \n",
    "    if is_duplicate_model(model):\n",
    "        slack.send(f\"‚ö† Duplicate skipped: {model['run_name']}\", \"warning\")\n",
    "        return None\n",
    "\n",
    "    print(f\"   üîÑ Registering model...\")\n",
    "    new_version = mlflow.register_model(model[\"model_uri\"], config.MODEL_NAME)\n",
    "    version = new_version.version\n",
    "\n",
    "    print(f\"   ‚úÖ Registered as version: {version}\")\n",
    "    slack.send(f\"‚úÖ Registered: {config.MODEL_NAME} v{version}\", \"success\")\n",
    "\n",
    "    try:\n",
    "        tags = {\n",
    "            \"run_name\": model[\"run_name\"],\n",
    "            \"run_id\": model[\"run_id\"],\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": str(round(model[\"primary_metric\"], 4)),\n",
    "            \"artifact_path\": config.ARTIFACT_PATH,\n",
    "            \"registered_timestamp\": datetime.now().isoformat(),\n",
    "            \"registered_by\": \"Automated Pipeline\",\n",
    "            \"params\": json.dumps(model[\"params\"]),\n",
    "            \"metrics\": json.dumps({k: round(v, 4) for k, v in model[\"metrics\"].items() if v is not None})\n",
    "        }\n",
    "\n",
    "        for key, value in tags.items():\n",
    "            client.set_model_version_tag(config.MODEL_NAME, version, key, value)\n",
    "\n",
    "        client.update_model_version(\n",
    "            name=config.MODEL_NAME,\n",
    "            version=version,\n",
    "            description=(\n",
    "                f\"üì¶ Automated Model Registration\\n\\n\"\n",
    "                f\"üîπ **Run Name:** {model['run_name']}\\n\"\n",
    "                f\"üîπ **Primary Metric ({config.PRIMARY_METRIC}):** {model['primary_metric']:.4f}\\n\"\n",
    "                f\"üîπ **Run ID:** {model['run_id']}\\n\"\n",
    "                f\"üìÖ **Registered:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "                f\"**Metrics:**\\n\"\n",
    "                + \"\\n\".join([f\"  ‚Ä¢ {k}: {v:.4f}\" for k, v in model[\"metrics\"].items() if v is not None])\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"   üè∑ Metadata and tags updated\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Failed to set tags: {e}\")\n",
    "\n",
    "    return version\n",
    "\n",
    "\n",
    "# ---------------------- LOG DECISION TO TABLE (UPDATED - NO DUPLICATES) ----------------------\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def log_decision(model, registered, version, reason):\n",
    "    \"\"\"Log registration decision to Delta table with MERGE to avoid duplicates\"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame([{\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"run_id\": model[\"run_id\"],\n",
    "            \"run_name\": model[\"run_name\"],\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"primary_metric\": config.PRIMARY_METRIC,\n",
    "            \"primary_metric_value\": model[\"primary_metric\"],\n",
    "            \"metrics_json\": json.dumps(model[\"metrics\"]),\n",
    "            \"params_json\": json.dumps(model[\"params\"]),\n",
    "            \"registered\": registered,\n",
    "            \"registered_version\": version,\n",
    "            \"reason\": reason\n",
    "        }])\n",
    "\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        ensure_table_exists(config.REGISTRATION_LOG_TABLE, spark_df.schema, spark)\n",
    "\n",
    "        try:\n",
    "            delta_table = DeltaTable.forName(spark, config.REGISTRATION_LOG_TABLE)\n",
    "\n",
    "            delta_table.alias(\"t\").merge(\n",
    "                spark_df.alias(\"s\"),\n",
    "                \"\"\"\n",
    "                t.run_id = s.run_id\n",
    "                AND t.model_name = s.model_name\n",
    "                AND t.registered_version = s.registered_version\n",
    "                \"\"\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "\n",
    "            print(f\"üîÅ MERGE successful ‚Üí No duplicate row added.\")\n",
    "\n",
    "        except Exception:\n",
    "            spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(config.REGISTRATION_LOG_TABLE)\n",
    "\n",
    "        print(f\"   üìÑ Logged to: {config.REGISTRATION_LOG_TABLE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Failed to log decision: {e}\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION ----------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\nüöÄ Starting Model Registration Pipeline...\\n\")\n",
    "    \n",
    "    runs = get_all_runs()\n",
    "\n",
    "    if not runs:\n",
    "        print(\"‚ùå No runs found in experiment!\")\n",
    "        return\n",
    "\n",
    "    registered_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for idx, model in enumerate(runs, start=1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(runs)}] Processing: {model['run_name']}\")\n",
    "        print(f\"   Primary Metric ({config.PRIMARY_METRIC}): {model['primary_metric']:.4f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        version = register_model(model)\n",
    "\n",
    "        if version:\n",
    "            log_decision(model, True, version, \"‚úî Registered successfully\")\n",
    "            registered_count += 1\n",
    "        else:\n",
    "            log_decision(model, False, None, \"‚ö† Duplicate - Skipped\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ REGISTRATION PIPELINE COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Models Registered: {registered_count}\")\n",
    "    print(f\"‚ö† Duplicates Skipped: {skipped_count}\")\n",
    "    print(f\"üìä Total Processed: {len(runs)}\")\n",
    "    print(f\"üì¶ Model Name: {config.MODEL_NAME}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    slack.send(f\"üìÅ Registration complete: {registered_count} registered, {skipped_count} skipped\", \"info\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
