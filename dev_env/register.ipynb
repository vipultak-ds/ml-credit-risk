{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6faded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ MODEL REGISTRATION SCRIPT \n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, BooleanType\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODEL REGISTRATION SYSTEM - AUTOMATED & DUPLICATE SAFE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD PIPELINE CONFIG ----------------------\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    print(\"‚úÖ pipeline_config.yml loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load config: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "\n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "        self.ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "\n",
    "        self.TOLERANCE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"tolerance\"]\n",
    "        self.METRICS_TO_COMPARE = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"metrics_to_compare\"]\n",
    "        self.DUPLICATE_CHECK_ENABLED = pipeline_cfg[\"registry\"][\"duplicate_detection\"][\"enabled\"]\n",
    "\n",
    "        self.REGISTRATION_LOG_TABLE = pipeline_cfg[\"tables\"][\"registration_log\"]\n",
    "\n",
    "        self.SLACK_WEBHOOK = None\n",
    "        try:\n",
    "            self.SLACK_WEBHOOK = dbutils.secrets.get(\"shared-scope\", \"SLACK_WEBHOOK_URL\")\n",
    "            print(\"üîê Slack webhook loaded\")\n",
    "        except:\n",
    "            print(\"‚ö† Slack webhook NOT configured (Optional)\")\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"\\nüìå Model Registry: {config.MODEL_NAME}\")\n",
    "print(f\"üìå Duplicate Logic: {'ENABLED' if config.DUPLICATE_CHECK_ENABLED else 'DISABLED'}\")\n",
    "print(f\"üìå Primary Metric: {config.PRIMARY_METRIC}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ---------------------- SLACK NOTIFIER ----------------------\n",
    "class SlackNotifier:\n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "\n",
    "    def send(self, message: str, level: str = \"info\"):\n",
    "        if not self.webhook_url:\n",
    "            return\n",
    "        emoji = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"warning\": \"‚ö†Ô∏è\", \"error\": \"‚ùå\"}.get(level, \"‚ÑπÔ∏è\")\n",
    "        payload = {\"text\": f\"{emoji} {message}\"}\n",
    "\n",
    "        try:\n",
    "            requests.post(self.webhook_url, json=payload, timeout=5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "slack = SlackNotifier(config.SLACK_WEBHOOK)\n",
    "\n",
    "\n",
    "# ---------------------- INIT SPARK + MLFLOW ----------------------\n",
    "spark = SparkSession.builder.appName(\"ModelRegistration\").getOrCreate()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(config.EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    raise Exception(f\"Experiment '{config.EXPERIMENT_NAME}' not found!\")\n",
    "\n",
    "\n",
    "# ---------------------- TABLE SCHEMA (FIXED) ----------------------\n",
    "def get_table_schema():\n",
    "    \"\"\"Define fixed schema for registration log table\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_name\", StringType(), True),\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"primary_metric\", StringType(), True),\n",
    "        StructField(\"primary_metric_value\", DoubleType(), True),\n",
    "        StructField(\"metrics_json\", StringType(), True),\n",
    "        StructField(\"params_json\", StringType(), True),\n",
    "        StructField(\"registered\", BooleanType(), True),\n",
    "        StructField(\"registered_version\", StringType(), True),\n",
    "        StructField(\"reason\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "# ---------------------- TABLE CREATION ----------------------\n",
    "def ensure_table_exists():\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {config.REGISTRATION_LOG_TABLE}\")\n",
    "        print(f\"üìå Table exists: {config.REGISTRATION_LOG_TABLE}\")\n",
    "    except:\n",
    "        print(f\"üÜï Creating Delta table: {config.REGISTRATION_LOG_TABLE}\")\n",
    "        schema = get_table_schema()\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(config.REGISTRATION_LOG_TABLE)\n",
    "        print(f\"‚úÖ Table created: {config.REGISTRATION_LOG_TABLE}\")\n",
    "\n",
    "\n",
    "# ---------------------- FETCH RUNS ----------------------\n",
    "def get_all_runs() -> List[Dict]:\n",
    "    print(\"\\nüìç Fetching experiment runs...\")\n",
    "    runs = client.search_runs(\n",
    "        [experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{config.PRIMARY_METRIC} DESC\"],\n",
    "        max_results=500\n",
    "    )\n",
    "\n",
    "    return [{\n",
    "        \"run_id\": run.info.run_id,\n",
    "        \"run_name\": run.info.run_name or \"unnamed_run\",\n",
    "        \"metrics\": {m: run.data.metrics.get(m) for m in config.METRICS_TO_COMPARE if m in run.data.metrics},\n",
    "        \"params\": run.data.params,\n",
    "        \"primary_metric\": run.data.metrics.get(config.PRIMARY_METRIC),\n",
    "        \"model_uri\": f\"runs:/{run.info.run_id}/{config.ARTIFACT_PATH}\"\n",
    "    } for run in runs]\n",
    "\n",
    "\n",
    "# ---------------------- DUPLICATE CHECK ----------------------\n",
    "def is_duplicate_model(new_model: Dict) -> bool:\n",
    "    if not config.DUPLICATE_CHECK_ENABLED:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{config.MODEL_NAME}'\")\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    for version in versions:\n",
    "        try:\n",
    "            run = client.get_run(version.run_id)\n",
    "\n",
    "            metrics_match = all(\n",
    "                abs((run.data.metrics.get(m) or 0) - (new_model[\"metrics\"].get(m) or 0)) <= config.TOLERANCE\n",
    "                for m in config.METRICS_TO_COMPARE\n",
    "            )\n",
    "\n",
    "            if metrics_match and run.data.params == new_model[\"params\"]:\n",
    "                print(f\"   ‚ö† Duplicate detected ‚Üí Matches v{version.version}\")\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# ---------------------- CHECK IF ALREADY LOGGED ----------------------\n",
    "def is_already_logged(run_id: str) -> bool:\n",
    "    try:\n",
    "        existing = spark.sql(f\"\"\"\n",
    "            SELECT run_id \n",
    "            FROM {config.REGISTRATION_LOG_TABLE} \n",
    "            WHERE run_id = '{run_id}'\n",
    "            LIMIT 1\n",
    "        \"\"\").count()\n",
    "\n",
    "        return existing > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ---------------------- REGISTER MODEL ----------------------\n",
    "def register_model(model: Dict):\n",
    "    if is_duplicate_model(model):\n",
    "        return None\n",
    "\n",
    "    print(f\"   üîÑ Registering model...\")\n",
    "    reg = mlflow.register_model(model[\"model_uri\"], config.MODEL_NAME)\n",
    "    version = reg.version\n",
    "\n",
    "    try:\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"run_id\", model[\"run_id\"])\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"run_name\", model[\"run_name\"])\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"primary_metric\", config.PRIMARY_METRIC)\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"primary_metric_value\", str(round(model[\"primary_metric\"], 4)))\n",
    "        client.set_model_version_tag(config.MODEL_NAME, version, \"registered_timestamp\", datetime.now().isoformat())\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Failed to set tags: {e}\")\n",
    "\n",
    "    print(f\"   ‚úÖ Registered as version: {version}\")\n",
    "    return version\n",
    "\n",
    "\n",
    "# ---------------------- LOG DECISION ----------------------\n",
    "def log_decision(model, registered, version, reason):\n",
    "\n",
    "    if is_already_logged(model[\"run_id\"]):\n",
    "        print(f\"   ‚ÑπÔ∏è Already logged in table, skipping duplicate entry\")\n",
    "        return\n",
    "\n",
    "    version_str = str(version) if version is not None else \"N/A\"\n",
    "\n",
    "    record = {\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"run_id\": model[\"run_id\"],\n",
    "        \"run_name\": model[\"run_name\"],\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"primary_metric\": config.PRIMARY_METRIC,\n",
    "        \"primary_metric_value\": float(model[\"primary_metric\"]) if model[\"primary_metric\"] else 0.0,\n",
    "        \"metrics_json\": json.dumps(model[\"metrics\"]),\n",
    "        \"params_json\": json.dumps(model[\"params\"]),\n",
    "        \"registered\": registered,\n",
    "        \"registered_version\": version_str,\n",
    "        \"reason\": reason\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([record])\n",
    "    spark_df = spark.createDataFrame(df, schema=get_table_schema())\n",
    "\n",
    "    try:\n",
    "        spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(config.REGISTRATION_LOG_TABLE)\n",
    "        print(f\"   üìÑ Logged to: {config.REGISTRATION_LOG_TABLE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Failed to log: {e}\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN ----------------------\n",
    "def main():\n",
    "    print(\"\\nüöÄ Starting Model Registration Pipeline...\\n\")\n",
    "\n",
    "    ensure_table_exists()\n",
    "\n",
    "    runs = get_all_runs()\n",
    "    if not runs:\n",
    "        print(\"‚ùå No runs found!\")\n",
    "        return\n",
    "\n",
    "    registered_count = 0\n",
    "    skipped_count = 0\n",
    "    processed_run_ids = set()\n",
    "\n",
    "    for idx, model in enumerate(runs, start=1):\n",
    "\n",
    "        if model['run_id'] in processed_run_ids:\n",
    "            continue\n",
    "\n",
    "        processed_run_ids.add(model['run_id'])\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{len(runs)}] Processing: {model['run_name']}\")\n",
    "        print(f\"   Primary Metric ({config.PRIMARY_METRIC}): {model['primary_metric']:.4f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # -------- FIXED BLOCK --------\n",
    "        if is_already_logged(model['run_id']):\n",
    "            print(f\"   ‚è≠ Skipped ‚Äî Already processed earlier (Run ID: {model['run_id']})\")\n",
    "            continue\n",
    "\n",
    "        version = register_model(model)\n",
    "        # -------- FIXED BLOCK END ----\n",
    "\n",
    "        if version:\n",
    "            log_decision(model, True, version, \"‚úî Registered successfully\")\n",
    "            slack.send(f\"‚úÖ Registered: {config.MODEL_NAME} v{version}\", \"success\")\n",
    "            registered_count += 1\n",
    "        else:\n",
    "            log_decision(model, False, None, \"‚ö† Duplicate - Skipped\")\n",
    "            slack.send(f\"‚ö† Duplicate skipped: {model['run_name']}\", \"warning\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ REGISTRATION PIPELINE COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Models Registered: {registered_count}\")\n",
    "    print(f\"‚ö† Duplicates Skipped: {skipped_count}\")\n",
    "    print(f\"üìä Total Processed: {len(runs)}\")\n",
    "    print(f\"üì¶ Model Name: {config.MODEL_NAME}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    slack.send(f\"üìÅ Registration complete: {registered_count} registered, {skipped_count} skipped\", \"info\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
