{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c571ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### TRAINING_script\n",
    "\n",
    "%pip install scikit-learn pyyaml xgboost\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ‚úÖ FIX: Suppress ALL warnings including threadpoolctl AttributeError\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=AttributeError, module=\"threadpoolctl\")\n",
    "\n",
    "# ‚úÖ FIX: Also suppress the specific threadpoolctl warning at import time\n",
    "import logging\n",
    "logging.getLogger(\"threadpoolctl\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ CREDIT RISK TRAINING - MULTI MODEL MODE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# üî• LOAD CONFIG FILES\n",
    "\n",
    "with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "    pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "with open(\"experiments_config.yml\", \"r\") as f:\n",
    "    experiments_cfg = yaml.safe_load(f)\n",
    "\n",
    "# üîπ ENSURE WIDGET EXISTS (REQUIRED FOR DATABRICKS JOBS)\n",
    "try:\n",
    "    dbutils.widgets.text(\n",
    "        \"MODELS_TO_TRAIN\",\n",
    "        \"\",\n",
    "        \"Models to Train\"\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# üî• GET MODELS TO TRAIN (Git / Job Variable)\n",
    "\n",
    "def get_models_to_train():\n",
    "    \"\"\"\n",
    "    ‚úÖ ENHANCED: Better validation and \"all\" keyword support\n",
    "    \"\"\"\n",
    "    # Get available models from experiments_config.yml\n",
    "    available_models = list(experiments_cfg.get(\"models\", {}).keys())\n",
    "    \n",
    "    if not available_models:\n",
    "        raise ValueError(\"‚ùå No models defined in experiments_config.yml\")\n",
    "    \n",
    "    # Try to get value from widget first, then environment\n",
    "    value = None\n",
    "    try:\n",
    "        value = dbutils.widgets.get(\"MODELS_TO_TRAIN\")\n",
    "        print(f\"üìå MODELS_TO_TRAIN from Widget: '{value}'\")\n",
    "    except:\n",
    "        value = os.getenv(\"MODELS_TO_TRAIN\", \"\")\n",
    "        print(f\"üìå MODELS_TO_TRAIN from ENV: '{value}'\")\n",
    "    \n",
    "    # Clean the value\n",
    "    if value:\n",
    "        value = value.strip()\n",
    "    \n",
    "    # ‚úÖ Handle \"None\" string explicitly\n",
    "    if not value or value == \"\" or value.lower() in [\"none\", \"null\", \"undefined\"]:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå MODELS_TO_TRAIN is not set!\\n\"\n",
    "            f\"   Available models in experiments_config.yml: {available_models}\\n\"\n",
    "            f\"   \\n\"\n",
    "            f\"   Set via Git CI/CD:\\n\"\n",
    "            f\"   - GitHub: Set variable MODELS_TO_TRAIN='random_forest,xgboost'\\n\"\n",
    "            f\"   - GitLab: Set CI/CD variable MODELS_TO_TRAIN='random_forest,xgboost'\\n\"\n",
    "            f\"   - Databricks Job: Pass as parameter\\n\"\n",
    "            f\"   \\n\"\n",
    "            f\"   Special keywords:\\n\"\n",
    "            f\"   - 'all' = train all available models\\n\"\n",
    "            f\"   \\n\"\n",
    "            f\"   Current value received: '{value}'\"\n",
    "        )\n",
    "    \n",
    "    # ‚úÖ Handle \"all\" keyword\n",
    "    if value.lower() == \"all\":\n",
    "        print(f\"‚úÖ Training ALL models: {available_models}\")\n",
    "        return available_models\n",
    "    \n",
    "    # Parse comma-separated values\n",
    "    models = [m.strip() for m in value.split(\",\") if m.strip()]\n",
    "    \n",
    "    if not models:\n",
    "        raise ValueError(f\"‚ùå No valid models found in MODELS_TO_TRAIN='{value}'\")\n",
    "    \n",
    "    # ‚úÖ Validate against experiments_config.yml\n",
    "    invalid_models = [m for m in models if m not in available_models]\n",
    "    \n",
    "    if invalid_models:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå Invalid model names: {invalid_models}\\n\"\n",
    "            f\"   Available in experiments_config.yml: {available_models}\\n\"\n",
    "            f\"   \\n\"\n",
    "            f\"   You tried to train: {models}\\n\"\n",
    "            f\"   Check your Git variable or experiments_config.yml\"\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Training selected models: {models}\")\n",
    "    return models\n",
    "\n",
    "# ‚úÖ CHANGE 2: Call validation early - before any MLflow operations\n",
    "try:\n",
    "    MODELS_TO_TRAIN = get_models_to_train()\n",
    "    print(f\"\\nüìã Models to train: {MODELS_TO_TRAIN}\\n\")\n",
    "except ValueError as e:\n",
    "    print(str(e))\n",
    "    dbutils.notebook.exit(\"FAILED: Invalid MODELS_TO_TRAIN configuration\")\n",
    "\n",
    "# üî• PIPELINE SETTINGS\n",
    "\n",
    "BASE_EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "MODEL_ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "RAW_INPUT_TABLE = pipeline_cfg[\"data\"][\"input_table\"]\n",
    "FEATURES = pipeline_cfg[\"data\"][\"features\"]\n",
    "LABEL_COL = pipeline_cfg[\"data\"][\"label\"]\n",
    "RUN_NAME_PREFIX = pipeline_cfg[\"experiment\"][\"run_name_prefix\"]\n",
    "\n",
    "# üî• LOAD DATA\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreditRiskTraining\").getOrCreate()\n",
    "df = spark.read.table(RAW_INPUT_TABLE).toPandas()\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[LABEL_COL]\n",
    "\n",
    "if y.dtype == \"object\":\n",
    "    y = y.map({\"yes\": 1, \"no\": 0}).astype(int)\n",
    "\n",
    "# üî• PREPROCESSING\n",
    "\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "        (\"numeric\", StandardScaler(), numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# üî• TRAIN-TEST SPLIT\n",
    "\n",
    "stratify_option = y if pipeline_cfg[\"data\"][\"split\"][\"stratify\"] else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=pipeline_cfg[\"data\"][\"split\"][\"test_size\"],\n",
    "    stratify=stratify_option,\n",
    "    random_state=pipeline_cfg[\"data\"][\"split\"][\"random_state\"]\n",
    ")\n",
    "\n",
    "# üî• MLflow SETUP\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "print(f\"üî¨ Setting MLflow experiment: {BASE_EXPERIMENT_NAME}\")\n",
    "mlflow.set_experiment(BASE_EXPERIMENT_NAME)\n",
    "print(f\"‚úÖ Experiment set successfully\\n\")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": XGBClassifier,\n",
    "    \"logistic_regression\": LogisticRegression\n",
    "}\n",
    "\n",
    "# üî• TRAIN LOOP\n",
    "\n",
    "for MODEL_TYPE in MODELS_TO_TRAIN:\n",
    "\n",
    "    if MODEL_TYPE not in MODEL_CLASSES:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {MODEL_TYPE} - model class not found\")\n",
    "        continue\n",
    "\n",
    "    if MODEL_TYPE not in experiments_cfg[\"models\"]:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {MODEL_TYPE} - not in experiments_config.yml\")\n",
    "        continue\n",
    "\n",
    "    ModelClass = MODEL_CLASSES[MODEL_TYPE]\n",
    "    EXPERIMENT_LIST = experiments_cfg[\"models\"][MODEL_TYPE][\"experiments\"]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéØ Training {MODEL_TYPE.upper()} - {len(EXPERIMENT_LIST)} experiments\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for exp in EXPERIMENT_LIST:\n",
    "\n",
    "        exp_name = f\"{RUN_NAME_PREFIX}_{MODEL_TYPE}_{exp['name']}\"\n",
    "        params = exp[\"params\"].copy()\n",
    "\n",
    "        with mlflow.start_run(run_name=exp_name):\n",
    "\n",
    "            model = ModelClass(**params)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocessing\", preprocessor),\n",
    "                (\"model\", model)\n",
    "            ])\n",
    "\n",
    "            start = time.time()\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            train_time = round(time.time() - start, 4)\n",
    "\n",
    "            train_pred = pipeline.predict(X_train)\n",
    "            train_accuracy = accuracy_score(y_train, train_pred)\n",
    "\n",
    "            start_inf = time.time()\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            inference_time = round(time.time() - start_inf, 4)\n",
    "\n",
    "            if hasattr(pipeline.named_steps[\"model\"], \"predict_proba\"):\n",
    "                y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_proba = None\n",
    "\n",
    "            metrics = {\n",
    "                \"test_accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"test_precision\": precision_score(y_test, y_pred),\n",
    "                \"test_recall\": recall_score(y_test, y_pred),\n",
    "                \"test_f1\": f1_score(y_test, y_pred),\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"train_time\": train_time,\n",
    "                \"inference_time\": inference_time\n",
    "            }\n",
    "\n",
    "            if y_proba is not None:\n",
    "                metrics[\"test_roc_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                mlflow.log_metric(k, v)\n",
    "\n",
    "            model_step = pipeline.named_steps[\"model\"]\n",
    "            if MODEL_TYPE == \"logistic_regression\" and hasattr(model_step, \"n_iter_\"):\n",
    "                mlflow.log_metric(\"lr_n_iterations\", int(np.max(model_step.n_iter_)))\n",
    "\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param(\"model_type\", MODEL_TYPE)\n",
    "            mlflow.log_param(\"experiment_name\", BASE_EXPERIMENT_NAME)\n",
    "\n",
    "            signature = infer_signature(X_train, pipeline.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline,\n",
    "                artifact_path=MODEL_ARTIFACT_PATH,\n",
    "                signature=signature,\n",
    "                input_example=X_train.head(5)\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ {exp_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ ALL MODELS TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
