{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee2ee0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üß™ UAT MODEL INFERENCE - NEW WORKFLOW (CONFIG-DRIVEN)\n",
    "# =============================================================================\n",
    "# Purpose: Validate staging model performance on UAT data\n",
    "# Compatible with: train.py ‚Üí model_registration.py ‚Üí uat_staging.py ‚Üí THIS SCRIPT\n",
    "# Prerequisites: Run uat_staging.py first to promote model to @Staging\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import traceback\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ UAT MODEL INFERENCE (NEW WORKFLOW)\")\n",
    "print(\"=\" * 80)\n",
    " \n",
    "# ‚úÖ LOAD PIPELINE CONFIGURATION\n",
    "\n",
    "print(\"\\nüìã Step 1: Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Configuration loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    " \n",
    "# ‚úÖ CONFIGURATION CLASS\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration manager - reads from pipeline_config.yml\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model configuration\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "        \n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.MODEL_TYPE = MODEL_TYPE\n",
    "        \n",
    "        # Aliases\n",
    "        self.STAGING_ALIAS = pipeline_cfg[\"aliases\"][\"staging\"]\n",
    "        self.PRODUCTION_ALIAS = pipeline_cfg[\"aliases\"][\"production\"]\n",
    "        \n",
    "        # Data configuration - Use PREPROCESSED data (same as train.py)\n",
    "        self.UAT_INPUT_TABLE = pipeline_cfg[\"data\"][\"preprocessed_table\"]\n",
    "        self.LABEL_COL = \"label\"  # Preprocessed data has 'label' column\n",
    "        \n",
    "        # Metrics configuration\n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        self.DIRECTION = pipeline_cfg[\"metrics\"][\"classification\"][\"direction\"]\n",
    "        self.TRACKED_METRICS = pipeline_cfg[\"metrics\"][\"classification\"][\"tracked_metrics\"]\n",
    "        \n",
    "        # UAT thresholds from classification config\n",
    "        self.UAT_THRESHOLDS = pipeline_cfg[\"uat\"][\"classification_thresholds\"]\n",
    "        \n",
    "        # Output table\n",
    "        self.UAT_RESULTS_TABLE = pipeline_cfg[\"tables\"][\"uat_results\"]\n",
    "        \n",
    "        # Slack notifications\n",
    "        self.SLACK_ENABLED = pipeline_cfg[\"notifications\"][\"enabled\"]\n",
    "        self.SLACK_WEBHOOK_URL = self._get_slack_webhook()\n",
    "        \n",
    "        print(f\"\\nüìä Configuration Summary:\")\n",
    "        print(f\"   Model Type: {self.MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Model Name: {self.MODEL_NAME}\")\n",
    "        print(f\"   Staging Alias: @{self.STAGING_ALIAS}\")\n",
    "        print(f\"   UAT Input: {self.UAT_INPUT_TABLE}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC}\")\n",
    "        print(f\"   Slack: {'ENABLED' if self.SLACK_WEBHOOK_URL else 'DISABLED'}\")\n",
    "    \n",
    "    def _get_slack_webhook(self) -> Optional[str]:\n",
    "        \"\"\"Safely retrieve Slack webhook URL from Databricks secrets\"\"\"\n",
    "        if not self.SLACK_ENABLED:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            scopes = [\"shared-scope\", \"dev-scope\", \"prod-scope\", \"ml-scope\"]\n",
    "            for scope in scopes:\n",
    "                try:\n",
    "                    webhook = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "                    if webhook and webhook.strip():\n",
    "                        print(f\"   ‚úÖ Slack webhook found in scope '{scope}'\")\n",
    "                        return webhook\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            print(\"   ‚ÑπÔ∏è  No Slack webhook found in secrets\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not access secrets: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize config\n",
    "config = Config()\n",
    "\n",
    "print(\"=\" * 80)\n",
    " \n",
    "# ‚úÖ SLACK NOTIFICATION HELPER\n",
    "\n",
    "class SlackNotifier:\n",
    "    \"\"\"Slack notification handler\"\"\"\n",
    "    \n",
    "    def __init__(self, webhook_url: Optional[str]):\n",
    "        self.webhook_url = webhook_url\n",
    "        self.enabled = webhook_url is not None and webhook_url.strip() != \"\"\n",
    "        \n",
    "    def send(self, message: str, level: str = \"info\", extra_fields: Optional[Dict] = None) -> bool:\n",
    "        \"\"\"Send Slack notification\"\"\"\n",
    "        if not self.enabled:\n",
    "            print(f\"üì¢ [SLACK DISABLED] {message}\")\n",
    "            return False\n",
    "        \n",
    "        emoji_map = {\n",
    "            \"info\": \"‚ÑπÔ∏è\",\n",
    "            \"success\": \"‚úÖ\",\n",
    "            \"warning\": \"‚ö†Ô∏è\",\n",
    "            \"error\": \"‚ùå\",\n",
    "            \"test\": \"üß™\"\n",
    "        }\n",
    "        \n",
    "        formatted_message = f\"{emoji_map.get(level, '‚ÑπÔ∏è')} *{message}*\"\n",
    "        \n",
    "        if extra_fields:\n",
    "            formatted_message += \"\\n\"\n",
    "            for key, value in extra_fields.items():\n",
    "                formatted_message += f\"\\n‚Ä¢ *{key}:* {value}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"text\": formatted_message,\n",
    "            \"username\": \"ML Pipeline Bot\",\n",
    "            \"icon_emoji\": \":test_tube:\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.webhook_url,\n",
    "                json=payload,\n",
    "                timeout=5\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"üì¢ Slack notification sent successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Slack error: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Slack notification failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize Slack notifier\n",
    "slack = SlackNotifier(config.SLACK_WEBHOOK_URL)\n",
    " \n",
    "# ‚úÖ INITIALIZE MLFLOW & SPARK\n",
    "\n",
    "print(\"\\nüîß Step 2: Initializing MLflow and Spark...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"UAT_Inference\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(\"‚úÖ MLflow and Spark initialized successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Send startup notification\n",
    "slack.send(\n",
    "    \"UAT Inference Pipeline Started\",\n",
    "    level=\"test\",\n",
    "    extra_fields={\n",
    "        \"Model\": config.MODEL_NAME,\n",
    "        \"Model Type\": config.MODEL_TYPE.upper(),\n",
    "        \"Alias\": f\"@{config.STAGING_ALIAS}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# üîß HELPER FUNCTION: CONVERT SPARK VECTOR TO NUMPY\n",
    "# =============================================================================\n",
    "def vector_to_array(v):\n",
    "    \"\"\"Convert PySpark ML Vector to numpy array\"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "    return v.toArray() if hasattr(v, 'toArray') else np.array(v)\n",
    " \n",
    "# üìã STEP 1: LOAD MODEL FROM STAGING\n",
    "\n",
    "def load_staging_model() -> Tuple[any, int, str]:\n",
    "    \"\"\"Load model from Unity Catalog using Staging alias\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìã STEP 1: Loading Model from @{config.STAGING_ALIAS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Looking for model: {config.MODEL_NAME}@{config.STAGING_ALIAS}\")\n",
    "        \n",
    "        # Get model version by alias\n",
    "        try:\n",
    "            model_version = client.get_model_version_by_alias(\n",
    "                config.MODEL_NAME, \n",
    "                config.STAGING_ALIAS\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå No model found with alias @{config.STAGING_ALIAS}\")\n",
    "            print(f\"üí° Please run uat_staging.py first to promote a model\")\n",
    "            raise Exception(f\"Model not found: {e}\")\n",
    "        \n",
    "        version = int(model_version.version)\n",
    "        run_id = model_version.run_id\n",
    "        status = model_version.status\n",
    "        \n",
    "        print(f\"‚úÖ Found model:\")\n",
    "        print(f\"   Version: v{version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Status: {status}\")\n",
    "        \n",
    "        # Get training metrics from tags\n",
    "        tags = model_version.tags\n",
    "        print(f\"\\nüìä Training Metrics (from tags):\")\n",
    "        for key, value in tags.items():\n",
    "            if key.startswith(\"metric_\"):\n",
    "                metric_name = key.replace(\"metric_\", \"\")\n",
    "                print(f\"   {metric_name}: {value}\")\n",
    "        \n",
    "        # Load the model\n",
    "        model_uri = f\"models:/{config.MODEL_NAME}@{config.STAGING_ALIAS}\"\n",
    "        print(f\"\\n‚è≥ Loading model...\")\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Model loaded successfully\")\n",
    "        \n",
    "        return model, version, run_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to load model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    " \n",
    "# üìã STEP 2: LOAD UAT DATA\n",
    "\n",
    "def load_uat_data() -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load UAT data from preprocessed Delta table\n",
    "    Same format as train.py uses - with PySpark Vectors\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 2: Loading UAT Data (Preprocessed)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìä Loading from: {config.UAT_INPUT_TABLE}\")\n",
    "        \n",
    "        # Read from Delta\n",
    "        df_spark = spark.read.format(\"delta\").table(config.UAT_INPUT_TABLE)\n",
    "        \n",
    "        # Check schema\n",
    "        print(f\"\\nüìä Schema:\")\n",
    "        df_spark.printSchema()\n",
    "        \n",
    "        # Convert to pandas\n",
    "        df = df_spark.toPandas()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "        print(f\"   Total rows: {len(df):,}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Extract features and labels (same as train.py)\n",
    "        print(f\"\\nüîÑ Converting PySpark Vectors to numpy arrays...\")\n",
    "        \n",
    "        # 'features' column contains PySpark Vectors, convert to numpy\n",
    "        X = np.array([vector_to_array(row) for row in df['features']])\n",
    "        y_true = df[config.LABEL_COL].values\n",
    "        \n",
    "        print(f\"‚úÖ Conversion complete!\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Labels shape: {y_true.shape}\")\n",
    "        \n",
    "        # Check label distribution\n",
    "        unique_labels, counts = np.unique(y_true, return_counts=True)\n",
    "        print(f\"\\nüìä Label distribution:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            label_name = \"Default\" if label == 1 else \"No Default\"\n",
    "            print(f\"      Class {int(label)} ({label_name}): {count:,} ({count / len(y_true) * 100:.1f}%)\")\n",
    "        \n",
    "        return df, X, y_true\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to load data: {e}\")\n",
    "        print(f\"üí° Verify table exists: {config.UAT_INPUT_TABLE}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    " \n",
    "# üìã STEP 3: RUN INFERENCE\n",
    "\n",
    "def run_inference(model: any, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Run model inference on UAT data\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 3: Running Inference\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"‚è≥ Generating predictions for {len(X):,} samples...\")\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Get probability predictions (for ROC-AUC)\n",
    "        # MLflow pyfunc models may not have predict_proba, so we handle this\n",
    "        try:\n",
    "            # Try to get the underlying sklearn model\n",
    "            if hasattr(model, '_model_impl') and hasattr(model._model_impl, 'predict_proba'):\n",
    "                y_pred_proba = model._model_impl.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                # Fallback: use predictions as probabilities (binary 0/1)\n",
    "                print(\"   ‚ÑπÔ∏è  predict_proba not available, using binary predictions\")\n",
    "                y_pred_proba = y_pred.astype(float)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not get probabilities: {e}\")\n",
    "            y_pred_proba = y_pred.astype(float)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        inference_time_ms = (inference_time / len(X)) * 1000  # ms per sample\n",
    "        \n",
    "        print(f\"\\n‚úÖ Inference complete\")\n",
    "        print(f\"   Predictions generated: {len(y_pred):,}\")\n",
    "        print(f\"   Total time: {inference_time:.2f}s\")\n",
    "        print(f\"   Time per sample: {inference_time_ms:.3f}ms\")\n",
    "        print(f\"   Prediction distribution:\\n{pd.Series(y_pred).value_counts()}\")\n",
    "        \n",
    "        return y_pred, y_pred_proba\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Inference failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    " \n",
    "# üìã STEP 4: CALCULATE METRICS\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: np.ndarray) -> Dict:\n",
    "    \"\"\"Calculate evaluation metrics for classification\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 4: Calculating Metrics\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Classification metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        # ROC-AUC (if probabilities available)\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not calculate ROC-AUC: {e}\")\n",
    "            metrics['roc_auc'] = None\n",
    "        \n",
    "        print(f\"üìä Classification Metrics:\")\n",
    "        print(f\"   Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"   F1 Score:  {metrics['f1']:.4f}\")\n",
    "        if metrics['roc_auc'] is not None:\n",
    "            print(f\"   ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(f\"\\nüìä Confusion Matrix:\")\n",
    "        print(f\"   [[TN={cm[0,0]:<6} FP={cm[0,1]:<6}]\")\n",
    "        print(f\"    [FN={cm[1,0]:<6} TP={cm[1,1]:<6}]]\")\n",
    "        \n",
    "        metrics['confusion_matrix'] = cm.tolist()\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Metric calculation failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    " \n",
    "# üìã STEP 5: VALIDATE UAT\n",
    "\n",
    "def validate_uat(metrics: Dict, model_version: int) -> Tuple[str, list]:\n",
    "    \"\"\"Validate model against UAT thresholds\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 5: UAT Validation\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüìè Validation Thresholds:\")\n",
    "    for metric_name, threshold_value in config.UAT_THRESHOLDS.items():\n",
    "        if not metric_name.startswith(\"max_\"):\n",
    "            print(f\"   {metric_name}: ‚â• {threshold_value}\")\n",
    "        else:\n",
    "            print(f\"   {metric_name}: ‚â§ {threshold_value}\")\n",
    "    \n",
    "    print(f\"\\nüìä Actual Performance:\")\n",
    "    \n",
    "    passed_checks = []\n",
    "    failed_checks = []\n",
    "    \n",
    "    # Map metric names from config to calculated metrics\n",
    "    metric_mapping = {\n",
    "        'min_accuracy': 'accuracy',\n",
    "        'min_f1': 'f1',\n",
    "        'min_roc_auc': 'roc_auc',\n",
    "        'min_recall': 'recall',\n",
    "        'max_inference_time_ms': 'inference_time_ms'  # Would need to track separately\n",
    "    }\n",
    "    \n",
    "    for threshold_name, threshold_value in config.UAT_THRESHOLDS.items():\n",
    "        # Map threshold name to metric name\n",
    "        metric_name = metric_mapping.get(threshold_name)\n",
    "        \n",
    "        if metric_name is None or metric_name not in metrics or metrics[metric_name] is None:\n",
    "            continue\n",
    "        \n",
    "        actual_value = metrics[metric_name]\n",
    "        \n",
    "        # Determine if threshold is min or max\n",
    "        if threshold_name.startswith(\"min_\"):\n",
    "            passed = actual_value >= threshold_value\n",
    "            operator = \">=\"\n",
    "        elif threshold_name.startswith(\"max_\"):\n",
    "            passed = actual_value <= threshold_value\n",
    "            operator = \"<=\"\n",
    "        else:\n",
    "            passed = actual_value >= threshold_value\n",
    "            operator = \">=\"\n",
    "        \n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        print(f\"   {threshold_name}: {actual_value:.4f} {operator} {threshold_value} {status}\")\n",
    "        \n",
    "        if passed:\n",
    "            passed_checks.append(threshold_name)\n",
    "        else:\n",
    "            failed_checks.append({\n",
    "                'metric': threshold_name,\n",
    "                'actual': actual_value,\n",
    "                'threshold': threshold_value,\n",
    "                'operator': operator\n",
    "            })\n",
    "    \n",
    "    # Determine overall status\n",
    "    if not failed_checks:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ‚úÖ UAT PASSED ‚úÖ‚úÖ\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"   Model v{model_version} is ready for production!\")\n",
    "        \n",
    "        slack.send(\n",
    "            \"UAT Validation PASSED\",\n",
    "            level=\"success\",\n",
    "            extra_fields={\n",
    "                \"Model\": config.MODEL_NAME,\n",
    "                \"Version\": f\"v{model_version}\",\n",
    "                \"Accuracy\": f\"{metrics.get('accuracy', 0):.4f}\",\n",
    "                \"F1 Score\": f\"{metrics.get('f1', 0):.4f}\",\n",
    "                \"Status\": \"Ready for Production\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return \"PASSED\", []\n",
    "    else:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚ùå‚ùå UAT FAILED ‚ùå‚ùå\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\"\\n   Failed checks ({len(failed_checks)}):\")\n",
    "        for check in failed_checks:\n",
    "            print(f\"   ‚Ä¢ {check['metric']}: {check['actual']:.4f} \"\n",
    "                  f\"{check['operator']} {check['threshold']}\")\n",
    "        \n",
    "        slack.send(\n",
    "            \"UAT Validation FAILED\",\n",
    "            level=\"error\",\n",
    "            extra_fields={\n",
    "                \"Model\": config.MODEL_NAME,\n",
    "                \"Version\": f\"v{model_version}\",\n",
    "                \"Failed Checks\": len(failed_checks),\n",
    "                \"Status\": \"Needs Retraining\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return \"FAILED\", failed_checks\n",
    " \n",
    "# üìã STEP 6: LOG RESULTS\n",
    "\n",
    "def log_results(\n",
    "    model_version: int,\n",
    "    run_id: str,\n",
    "    metrics: Dict,\n",
    "    status: str,\n",
    "    failed_checks: list\n",
    ") -> None:\n",
    "    \"\"\"Log UAT results to Delta table\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìã STEP 6: Logging Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        table_exists = False\n",
    "        try:\n",
    "            spark.table(config.UAT_RESULTS_TABLE)\n",
    "            table_exists = True\n",
    "            print(f\"   Table exists: Yes\")\n",
    "        except Exception:\n",
    "            print(f\"   Table exists: No (will be created)\")\n",
    "        \n",
    "        # Prepare result data\n",
    "        result_data = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"model_type\": config.MODEL_TYPE,\n",
    "            \"model_version\": str(model_version),\n",
    "            \"run_id\": run_id,\n",
    "            \"uat_status\": status,\n",
    "            \n",
    "            # Individual metrics\n",
    "            \"accuracy\": float(metrics.get('accuracy', 0)),\n",
    "            \"precision\": float(metrics.get('precision', 0)),\n",
    "            \"recall\": float(metrics.get('recall', 0)),\n",
    "            \"f1\": float(metrics.get('f1', 0)),\n",
    "            \"roc_auc\": float(metrics.get('roc_auc', 0)) if metrics.get('roc_auc') else None,\n",
    "            \n",
    "            # Metadata\n",
    "            \"all_metrics_json\": json.dumps({k: float(v) if v is not None else None \n",
    "                                           for k, v in metrics.items() \n",
    "                                           if k != 'confusion_matrix'}),\n",
    "            \"thresholds_json\": json.dumps(config.UAT_THRESHOLDS),\n",
    "            \"failed_checks_json\": json.dumps(failed_checks) if failed_checks else None,\n",
    "            \"num_failed_checks\": len(failed_checks),\n",
    "            \n",
    "            # Confusion matrix\n",
    "            \"confusion_matrix_json\": json.dumps(metrics.get('confusion_matrix', []))\n",
    "        }\n",
    "        \n",
    "        result_df = pd.DataFrame([result_data])\n",
    "        spark_df = spark.createDataFrame(result_df)\n",
    "        \n",
    "        # Write to Delta table\n",
    "        if table_exists:\n",
    "            spark_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "                config.UAT_RESULTS_TABLE\n",
    "            )\n",
    "        else:\n",
    "            spark_df.write.mode(\"append\").saveAsTable(config.UAT_RESULTS_TABLE)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Results logged successfully\")\n",
    "        print(f\"   Output Table: {config.UAT_RESULTS_TABLE}\")\n",
    "        print(f\"   Model: {config.MODEL_NAME}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   Status: {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed to log results: {e}\")\n",
    "        traceback.print_exc()\n",
    " \n",
    "# üé¨ MAIN EXECUTION\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main UAT inference pipeline\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üé¨ STARTING UAT INFERENCE PIPELINE\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Load model\n",
    "        model, model_version, run_id = load_staging_model()\n",
    "        \n",
    "        # Step 2: Load UAT data\n",
    "        df, X, y_true = load_uat_data()\n",
    "        \n",
    "        # Step 3: Run inference\n",
    "        y_pred, y_pred_proba = run_inference(model, X)\n",
    "        \n",
    "        # Step 4: Calculate metrics\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_pred_proba)\n",
    "        \n",
    "        # Step 5: Validate UAT\n",
    "        status, failed_checks = validate_uat(metrics, model_version)\n",
    "        \n",
    "        # Step 6: Log results\n",
    "        log_results(model_version, run_id, metrics, status, failed_checks)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ú® UAT INFERENCE COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüìä Final Summary:\")\n",
    "        print(f\"   Model: {config.MODEL_NAME}\")\n",
    "        print(f\"   Model Type: {config.MODEL_TYPE.upper()}\")\n",
    "        print(f\"   Version: v{model_version}\")\n",
    "        print(f\"   UAT Status: {status}\")\n",
    "        print(f\"\\n   Key Metrics:\")\n",
    "        print(f\"     ‚Ä¢ Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ F1 Score:  {metrics['f1']:.4f}\")\n",
    "        if metrics.get('roc_auc'):\n",
    "            print(f\"     ‚Ä¢ ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        if status == \"PASSED\":\n",
    "            print(f\"\\nüìå Next Step:\")\n",
    "            print(f\"   ‚úÖ Model is ready for production promotion\")\n",
    "            print(f\"   Run production_promotion.py to deploy\")\n",
    "        else:\n",
    "            print(f\"\\nüìå Next Step:\")\n",
    "            print(f\"   ‚ùå Model needs improvement\")\n",
    "            print(f\"   Failed checks: {len(failed_checks)}\")\n",
    "            print(f\"   Review metrics and retrain with better hyperparameters\")\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Save task values for workflow\n",
    "        try:\n",
    "            dbutils.jobs.taskValues.set(key=\"uat_status\", value=status)\n",
    "            dbutils.jobs.taskValues.set(key=\"model_version\", value=model_version)\n",
    "            dbutils.jobs.taskValues.set(key=\"accuracy\", value=float(metrics['accuracy']))\n",
    "            dbutils.jobs.taskValues.set(key=\"f1_score\", value=float(metrics['f1']))\n",
    "            dbutils.jobs.taskValues.set(key=\"num_failed_checks\", value=len(failed_checks))\n",
    "            print(\"‚úÖ Task values saved for workflow\")\n",
    "        except:\n",
    "            print(\"‚ÑπÔ∏è  Not running in workflow - skipping task values\")\n",
    "        \n",
    "        # Send final summary notification\n",
    "        slack.send(\n",
    "            \"UAT Pipeline Completed\",\n",
    "            level=\"success\" if status == \"PASSED\" else \"warning\",\n",
    "            extra_fields={\n",
    "                \"Model\": config.MODEL_NAME,\n",
    "                \"Version\": f\"v{model_version}\",\n",
    "                \"Status\": status,\n",
    "                \"Accuracy\": f\"{metrics['accuracy']:.4f}\",\n",
    "                \"F1\": f\"{metrics['f1']:.4f}\",\n",
    "                \"Next Step\": \"Production\" if status == \"PASSED\" else \"Retrain\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå UAT INFERENCE FAILED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        slack.send(\n",
    "            \"UAT Pipeline Failed\",\n",
    "            level=\"error\",\n",
    "            extra_fields={\n",
    "                \"Model\": config.MODEL_NAME,\n",
    "                \"Error\": str(e)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    " \n",
    "# ‚úÖ EXECUTE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
