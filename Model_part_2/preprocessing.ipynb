{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b093d7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, round, regexp_replace, trim\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow\n",
    "\n",
    "# CONFIG\n",
    "INPUT_DELTA_TABLE = \"workspace.new.credit\"\n",
    "OUTPUT_DELTA_TABLE = \"workspace.new.credit_preprocessed\"\n",
    "MODEL_REGISTRY_PATH = \"/Volumes/workspace/new/ml_feature_artifacts/preprocessing_pipeline\"\n",
    "\n",
    "# SPARK INITIALIZATION\n",
    "def initialize_spark():\n",
    "    if 'spark' in globals() and isinstance(globals()['spark'], SparkSession):\n",
    "        return globals()['spark']\n",
    "    return SparkSession.builder.appName(\"CreditRiskPreprocessing\").getOrCreate()\n",
    "\n",
    "# STEP 1: DATA INGESTION\n",
    "\n",
    "def ingest_data(spark: SparkSession, table_name: str):\n",
    "    print(f\"üì• Loading: {table_name}\")\n",
    "    \n",
    "    credit_df = spark.read.format(\"delta\").table(table_name)\n",
    "\n",
    "    feature_cols = [\n",
    "        'checking_balance', 'months_loan_duration', 'credit_history', 'purpose',\n",
    "        'amount', 'savings_balance', 'employment_duration', 'percent_of_income',\n",
    "        'years_at_residence', 'age', 'other_credit', 'housing',\n",
    "        'existing_loans_count', 'job', 'dependents', 'phone'\n",
    "    ]\n",
    "    \n",
    "    df = credit_df.select(\"id\", \"start_date\", \"end_date\", *feature_cols, col(\"default\").alias(\"label\"))\n",
    "    print(f\"‚úÖ Loaded Rows: {df.count():,}\")\n",
    "    return df\n",
    "\n",
    "# NEW STEP: FETCH ONLY NEW RECORDS (INCREMENTAL LOGIC)\n",
    "\n",
    "def fetch_new_records(spark: SparkSession, raw_df, output_table: str):\n",
    "    print(\"üîç Checking for new records...\")\n",
    "\n",
    "    try:\n",
    "        existing_df = spark.read.table(output_table).select(\"id\", \"start_date\", \"end_date\").dropDuplicates()\n",
    "\n",
    "        new_df = raw_df.join(\n",
    "            existing_df,\n",
    "            on=[\"id\", \"start_date\", \"end_date\"],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        print(f\"üÜï New records found: {new_df.count():,}\")\n",
    "        return new_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Output table not found or unreadable. Full load will run. Reason: {e}\")\n",
    "        return raw_df\n",
    "\n",
    "# STEP 2: UNIT CLEANUP\n",
    "\n",
    "def cleanup_units(df):\n",
    "    print(\"üßπ Cleaning units...\")\n",
    "\n",
    "    df = df.withColumn(\"checking_balance\", trim(regexp_replace(col(\"checking_balance\"), r\"\\s*DM\\s*$\", \"\")))\n",
    "    df = df.withColumn(\"savings_balance\", trim(regexp_replace(col(\"savings_balance\"), r\"\\s*DM\\s*$\", \"\")))\n",
    "    df = df.withColumn(\"employment_duration\", trim(regexp_replace(col(\"employment_duration\"), r\"\\s*years?\\s*$\", \"\")))\n",
    "\n",
    "    df = df.replace(\"\", \"unknown\")\n",
    "    \n",
    "    print(\"‚úÖ Unit cleanup done\")\n",
    "    return df\n",
    "\n",
    "# STEP 3: DATA PREPARATION\n",
    "\n",
    "def prepare_data(df):\n",
    "    print(\"üìä Preparing data...\")\n",
    "\n",
    "    df = df.withColumn(\"label\", when(col(\"label\") == \"yes\", 1.0).otherwise(0.0))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"monthly_income\",\n",
    "        round(\n",
    "            when(\n",
    "                (col(\"percent_of_income\") > 0) & (col(\"months_loan_duration\") > 0),\n",
    "                (col(\"amount\") / col(\"months_loan_duration\")) * (100 / col(\"percent_of_income\"))\n",
    "            ).otherwise(None),\n",
    "            2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Preparation complete\")\n",
    "    return df\n",
    "\n",
    "# STEP 4: ORDINAL ENCODING\n",
    "\n",
    "def ordinal_encoding(df):\n",
    "    print(\"üî¢ Applying ordinal encoding...\")\n",
    "\n",
    "    ordinal_config = {\n",
    "        'checking_balance': ['< 0', '1 - 200', '> 200', 'unknown'],\n",
    "        'savings_balance': ['< 100', '100 - 500', '500 - 1000', '> 1000', 'unknown'],\n",
    "        'employment_duration': ['unemployed', '< 1', '1 - 4', '4 - 7', '> 7', 'unknown'],\n",
    "        'credit_history': ['critical', 'poor', 'good', 'very good', 'perfect']\n",
    "    }\n",
    "\n",
    "    for col_name, categories in ordinal_config.items():\n",
    "        expr = None\n",
    "        for idx, cat in enumerate(categories):\n",
    "            expr = when(col(col_name) == cat, float(idx)) if expr is None else expr.when(col(col_name) == cat, float(idx))\n",
    "        df = df.withColumn(col_name, expr.otherwise(float(len(categories))))\n",
    "\n",
    "    print(\"‚úÖ Ordinal encoding done\")\n",
    "    return df\n",
    "\n",
    "# STEP 5: ONE-HOT ENCODING\n",
    "\n",
    "def onehot_encoding(df):\n",
    "    print(\"üî• One-hot encoding...\")\n",
    "\n",
    "    nominal_cols = ['purpose', 'other_credit', 'housing', 'job', 'phone']\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") for c in nominal_cols]\n",
    "    encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_vec\", dropLast=True) for c in nominal_cols]\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders)\n",
    "    model = pipeline.fit(df)\n",
    "    df = model.transform(df)\n",
    "\n",
    "    df = df.drop(*nominal_cols, *[f\"{c}_index\" for c in nominal_cols])\n",
    "\n",
    "    print(\"‚úÖ One-hot encoding done\")\n",
    "    return df, model\n",
    "\n",
    "# STEP 6: STANDARD SCALING\n",
    "\n",
    "def apply_standard_scaling(df):\n",
    "    print(\"üìè Scaling features...\")\n",
    "\n",
    "    exclude_cols = [\"id\", \"start_date\", \"end_date\", \"label\"]\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"unscaled_features\")\n",
    "    scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    model = pipeline.fit(df)\n",
    "    df = model.transform(df).select(\"id\", \"start_date\", \"end_date\", \"features\", \"label\")\n",
    "\n",
    "    print(\"‚úÖ Scaling complete\")\n",
    "    return df, model\n",
    "\n",
    "# STEP 7: SAVE TO DELTA TABLE\n",
    "\n",
    "def save_to_delta(df, table_name: str):\n",
    "    print(f\"üíæ Saving to Delta table: {table_name}\")\n",
    "    \n",
    "    # Append mode - add only new data\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "    \n",
    "    row_count = df.count()\n",
    "    print(f\"‚úÖ Saved {row_count:,} rows to {table_name}\")\n",
    "    \n",
    "    return row_count\n",
    "\n",
    "# STEP 8: SAVE PIPELINE MODELS (OPTIONAL - FOR INFERENCE)\n",
    "\n",
    "def save_pipeline_models(onehot_model, scaler_model, model_path: str):\n",
    "    print(f\"üíæ Saving preprocessing pipeline to: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Save OneHot pipeline\n",
    "        onehot_model.write().overwrite().save(f\"{model_path}/onehot_pipeline\")\n",
    "        \n",
    "        # Save Scaler pipeline\n",
    "        scaler_model.write().overwrite().save(f\"{model_path}/scaler_pipeline\")\n",
    "        \n",
    "        print(\"‚úÖ Pipeline models saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not save pipeline models: {e}\")\n",
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ CREDIT RISK PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    spark = initialize_spark()\n",
    "\n",
    "    # Step 1: Load full raw data\n",
    "    df = ingest_data(spark, INPUT_DELTA_TABLE)\n",
    "\n",
    "    # NEW: Fetch only new records (incremental)\n",
    "    df = fetch_new_records(spark, df, OUTPUT_DELTA_TABLE)\n",
    "\n",
    "    # If no new records, stop safely\n",
    "    new_count = df.count()\n",
    "    if new_count == 0:\n",
    "        print(\"‚úÖ No new records found. Output table will not be updated.\")\n",
    "        dbutils.notebook.exit(\"No new data - stopping pipeline\")\n",
    "\n",
    "\n",
    "    # Step 2-4: Basic preprocessing\n",
    "    df = cleanup_units(df)\n",
    "    df = prepare_data(df)\n",
    "    df = ordinal_encoding(df)\n",
    "    \n",
    "    # Step 5: One-hot encoding (returns model too)\n",
    "    df, onehot_model = onehot_encoding(df)\n",
    "    \n",
    "    # Step 6: Standard scaling (returns model too)\n",
    "    processed_df, scaler_model = apply_standard_scaling(df)\n",
    "\n",
    "    # Step 7: Save preprocessed data to Delta table\n",
    "    row_count = save_to_delta(processed_df, OUTPUT_DELTA_TABLE)\n",
    "    \n",
    "    # Step 8: Save pipeline models (optional - for future inference)\n",
    "    save_pipeline_models(onehot_model, scaler_model, MODEL_REGISTRY_PATH)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úÖ Preprocessed data saved to: {OUTPUT_DELTA_TABLE}\")\n",
    "    print(f\"‚úÖ Total rows: {row_count:,}\")\n",
    "    print(f\"‚úÖ Pipeline models saved to: {MODEL_REGISTRY_PATH}\")\n",
    "    print(\"\\nüìä Schema:\")\n",
    "    processed_df.printSchema()\n",
    "    print(\"\\nüìã Sample data:\")\n",
    "    processed_df.display(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
