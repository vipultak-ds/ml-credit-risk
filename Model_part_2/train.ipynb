{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c3f0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### TRAINING_script (ZIPPED Hyperparameter Runs from YAML + Evaluation Logging in same file)\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ CREDIT RISK TRAINING - MULTI MODEL MODE (ZIPPED PARAMS + EVAL LOGGING)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------------------- LOAD CONFIG FILES ----------------------\n",
    "with open(\"pipeline_config.yml\", \"r\") as f:\n",
    "    pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "with open(\"experiments_config.yml\", \"r\") as f:\n",
    "    experiments_cfg = yaml.safe_load(f)\n",
    "\n",
    "# ---------------------- INIT SPARK ----------------------\n",
    "spark = SparkSession.builder.appName(\"CreditRiskTraining\").getOrCreate()\n",
    "\n",
    "# ---------------------- EVALUATION CONFIG (FROM pipeline_config.yml) ----------------------\n",
    "EVAL_TABLE = pipeline_cfg[\"tables\"][\"evaluation_log\"]\n",
    "TRACKED_METRICS = pipeline_cfg[\"metrics\"][\"classification\"][\"tracked_metrics\"]\n",
    "\n",
    "DUPLICATE_CFG = pipeline_cfg.get(\"tables\", {}).get(\"duplicate_handling\", {})\n",
    "DUPLICATE_ENABLED = DUPLICATE_CFG.get(\"enabled\", True)\n",
    "\n",
    "print(f\"‚úÖ Evaluation Log Table: {EVAL_TABLE}\")\n",
    "print(f\"‚úÖ Tracked Metrics: {TRACKED_METRICS}\")\n",
    "print(f\"‚úÖ Duplicate Handling Enabled: {DUPLICATE_ENABLED}\")\n",
    "\n",
    "# ---------------------- EVALUATION TABLE FUNCTIONS ----------------------\n",
    "def create_eval_table_if_not_exists():\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {EVAL_TABLE} (\n",
    "        model_name STRING,\n",
    "        model_type STRING,\n",
    "        run_id STRING,\n",
    "        experiment_name STRING,\n",
    "        created_timestamp TIMESTAMP,\n",
    "        hyperparameters STRING,\n",
    "        metrics STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\")\n",
    "    print(f\"‚úÖ Evaluation table ready: {EVAL_TABLE}\")\n",
    "\n",
    "\n",
    "def is_duplicate(model_type: str, experiment_name: str, hyper_json: str) -> bool:\n",
    "    \"\"\"\n",
    "    Duplicate means:\n",
    "    same model_type + experiment_name + hyperparameters already exists\n",
    "    \"\"\"\n",
    "    if not DUPLICATE_ENABLED:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        df = spark.read.table(EVAL_TABLE).filter(\n",
    "            (col(\"model_type\") == model_type) &\n",
    "            (col(\"experiment_name\") == experiment_name) &\n",
    "            (col(\"hyperparameters\") == hyper_json)\n",
    "        )\n",
    "        return df.limit(1).count() > 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Duplicate check skipped (table read error): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def log_run_to_table(model_name, model_type, run_id, experiment_name, hyperparams, metrics):\n",
    "    \"\"\"\n",
    "    Stores:\n",
    "    - hyperparameters as JSON string\n",
    "    - metrics as JSON string (only tracked metrics)\n",
    "    - avoids duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_metrics = {k: metrics.get(k, None) for k in TRACKED_METRICS}\n",
    "\n",
    "    hyper_json = json.dumps(hyperparams, sort_keys=True)\n",
    "    metrics_json = json.dumps(filtered_metrics, sort_keys=True)\n",
    "\n",
    "    if is_duplicate(model_type, experiment_name, hyper_json):\n",
    "        print(f\"‚ö†Ô∏è Duplicate row detected. Skipping insert for run_id={run_id}\")\n",
    "        return\n",
    "\n",
    "    row = [{\n",
    "        \"model_name\": model_name,\n",
    "        \"model_type\": model_type,\n",
    "        \"run_id\": run_id,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"created_timestamp\": datetime.utcnow(),\n",
    "        \"hyperparameters\": hyper_json,\n",
    "        \"metrics\": metrics_json\n",
    "    }]\n",
    "\n",
    "    df = spark.createDataFrame(row)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(EVAL_TABLE)\n",
    "\n",
    "    print(f\"‚úÖ Logged evaluation row for run_id={run_id}\")\n",
    "\n",
    "# ---------------------- GET MODELS TO TRAIN ----------------------\n",
    "def get_models_to_train():\n",
    "    available_models = list(experiments_cfg.get(\"models\", {}).keys())\n",
    "    if not available_models:\n",
    "        raise ValueError(\"‚ùå No models defined in experiments_config.yml\")\n",
    "\n",
    "    print(f\"‚úÖ Training ALL models: {available_models}\")\n",
    "    return available_models\n",
    "\n",
    "MODELS_TO_TRAIN = get_models_to_train()\n",
    "\n",
    "# ---------------------- PIPELINE SETTINGS ----------------------\n",
    "BASE_EXPERIMENT_NAME = pipeline_cfg[\"experiment\"][\"name\"]\n",
    "MODEL_ARTIFACT_PATH = pipeline_cfg[\"experiment\"][\"artifact_path\"]\n",
    "RAW_INPUT_TABLE = pipeline_cfg[\"data\"][\"input_table\"]\n",
    "FEATURES = pipeline_cfg[\"data\"][\"features\"]\n",
    "LABEL_COL = pipeline_cfg[\"data\"][\"label\"]\n",
    "RUN_NAME_PREFIX = pipeline_cfg[\"experiment\"][\"run_name_prefix\"]\n",
    "\n",
    "# ---------------------- LOAD DATA ----------------------\n",
    "df = spark.read.table(RAW_INPUT_TABLE).toPandas()\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[LABEL_COL]\n",
    "\n",
    "if y.dtype == \"object\":\n",
    "    y = y.map({\"yes\": 1, \"no\": 0}).astype(int)\n",
    "\n",
    "# ---------------------- PREPROCESSING ----------------------\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "        (\"numeric\", StandardScaler(), numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------------- TRAIN-TEST SPLIT ----------------------\n",
    "stratify_option = y if pipeline_cfg[\"data\"][\"split\"][\"stratify\"] else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=pipeline_cfg[\"data\"][\"split\"][\"test_size\"],\n",
    "    stratify=stratify_option,\n",
    "    random_state=pipeline_cfg[\"data\"][\"split\"][\"random_state\"]\n",
    ")\n",
    "\n",
    "# ---------------------- MLFLOW SETUP ----------------------\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"random_forest\": RandomForestClassifier\n",
    "}\n",
    "\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"random_forest\": \"RF\"\n",
    "}\n",
    "\n",
    "def get_model_short_name(model_type):\n",
    "    if model_type in MODEL_SHORT_NAMES:\n",
    "        return MODEL_SHORT_NAMES[model_type]\n",
    "    words = model_type.split(\"_\")\n",
    "    return \"\".join([w[0].upper() for w in words if w])\n",
    "\n",
    "# ---------------------- ZIPPED PARAM GENERATOR ----------------------\n",
    "def generate_param_combinations(hyperparam_dict: dict):\n",
    "    \"\"\"\n",
    "    Zipped mode:\n",
    "    Run 1 -> first value of each list\n",
    "    Run 2 -> second value of each list\n",
    "    ...\n",
    "    Total runs = minimum length of all lists\n",
    "    \"\"\"\n",
    "    if not hyperparam_dict:\n",
    "        return []\n",
    "\n",
    "    for k, v in hyperparam_dict.items():\n",
    "        if not isinstance(v, list):\n",
    "            raise ValueError(f\"‚ùå Hyperparameter '{k}' must be a list. Found: {type(v)}\")\n",
    "\n",
    "    n_runs = min(len(v) for v in hyperparam_dict.values())\n",
    "\n",
    "    combos = []\n",
    "    for i in range(n_runs):\n",
    "        combo = {k: hyperparam_dict[k][i] for k in hyperparam_dict.keys()}\n",
    "        combos.append(combo)\n",
    "\n",
    "    return combos\n",
    "\n",
    "# ---------------------- CREATE EVAL TABLE ONCE ----------------------\n",
    "create_eval_table_if_not_exists()\n",
    "\n",
    "# ---------------------- TRAIN LOOP ----------------------\n",
    "for MODEL_TYPE in MODELS_TO_TRAIN:\n",
    "\n",
    "    if MODEL_TYPE not in MODEL_CLASSES:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {MODEL_TYPE} - model class not found\")\n",
    "        continue\n",
    "\n",
    "    if MODEL_TYPE not in experiments_cfg[\"models\"]:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {MODEL_TYPE} - not in experiments_config.yml\")\n",
    "        continue\n",
    "\n",
    "    model_short = get_model_short_name(MODEL_TYPE)\n",
    "    MODEL_EXPERIMENT_NAME = f\"{BASE_EXPERIMENT_NAME}_{model_short}\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üî¨ Setting experiment: {MODEL_EXPERIMENT_NAME}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    mlflow.set_experiment(MODEL_EXPERIMENT_NAME)\n",
    "\n",
    "    ModelClass = MODEL_CLASSES[MODEL_TYPE]\n",
    "\n",
    "    hyperparams = experiments_cfg[\"models\"][MODEL_TYPE].get(\"hyperparameters\", {})\n",
    "    if not hyperparams:\n",
    "        print(f\"‚ö†Ô∏è No hyperparameters found for {MODEL_TYPE}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    PARAM_COMBINATIONS = generate_param_combinations(hyperparams)\n",
    "\n",
    "    print(f\"üéØ Training {MODEL_TYPE.upper()} - {len(PARAM_COMBINATIONS)} runs (ZIPPED MODE)\\n\")\n",
    "\n",
    "    for idx, params in enumerate(PARAM_COMBINATIONS, start=1):\n",
    "\n",
    "        exp_name = f\"{RUN_NAME_PREFIX}_{MODEL_TYPE}_run_{idx}\"\n",
    "\n",
    "        with mlflow.start_run(run_name=exp_name) as run:\n",
    "\n",
    "            model = ModelClass(**params)\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocessing\", preprocessor),\n",
    "                (\"model\", model)\n",
    "            ])\n",
    "\n",
    "            # ‚úÖ TRAIN\n",
    "            start = time.time()\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            train_time = round(time.time() - start, 4)\n",
    "\n",
    "            # ‚úÖ TRAIN METRICS\n",
    "            train_pred = pipeline.predict(X_train)\n",
    "            train_accuracy = accuracy_score(y_train, train_pred)\n",
    "\n",
    "            # ‚úÖ INFERENCE\n",
    "            start_inf = time.time()\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            inference_time = round(time.time() - start_inf, 4)\n",
    "\n",
    "            if hasattr(pipeline.named_steps[\"model\"], \"predict_proba\"):\n",
    "                y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_proba = None\n",
    "\n",
    "            metrics = {\n",
    "                \"test_accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"test_precision\": precision_score(y_test, y_pred),\n",
    "                \"test_recall\": recall_score(y_test, y_pred),\n",
    "                \"test_f1\": f1_score(y_test, y_pred),\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"train_time\": train_time,\n",
    "                \"inference_time\": inference_time\n",
    "            }\n",
    "\n",
    "            if y_proba is not None:\n",
    "                metrics[\"test_roc_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "            # ‚úÖ Log metrics to MLflow\n",
    "            for k, v in metrics.items():\n",
    "                mlflow.log_metric(k, v)\n",
    "\n",
    "            # ‚úÖ Log params to MLflow\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param(\"model_type\", MODEL_TYPE)\n",
    "            mlflow.log_param(\"experiment_name\", MODEL_EXPERIMENT_NAME)\n",
    "\n",
    "            # ‚úÖ Log model to MLflow\n",
    "            signature = infer_signature(X_train, pipeline.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline,\n",
    "                artifact_path=MODEL_ARTIFACT_PATH,\n",
    "                signature=signature,\n",
    "                input_example=X_train.head(5)\n",
    "            )\n",
    "\n",
    "            # ‚úÖ Store evaluation row in Delta table (AFTER TRAINING)\n",
    "            log_run_to_table(\n",
    "                model_name=exp_name,\n",
    "                model_type=MODEL_TYPE,\n",
    "                run_id=run.info.run_id,\n",
    "                experiment_name=MODEL_EXPERIMENT_NAME,\n",
    "                hyperparams=params,\n",
    "                metrics=metrics\n",
    "            )\n",
    "\n",
    "            print(f\"   ‚úÖ {exp_name} | params={params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ ALL MODELS TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
