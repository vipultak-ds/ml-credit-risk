{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee2ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     accuracy_score, \n\u001b[32m     16\u001b[39m     precision_score, \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     confusion_matrix\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorUDT, Vectors\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# üß™ UAT MODEL INFERENCE - NEW WORKFLOW (CONFIG-DRIVEN)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import traceback\n",
    "import yaml\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ UAT MODEL INFERENCE (NEW WORKFLOW)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# LOAD CONFIG\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nüìã Step 1: Loading configuration from pipeline_config.yml...\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "\n",
    "    config_path = \"/Workspace/Repos/vipultak7171@gmail.com/ml-credit-risk/dev_env/pipeline_config.yml\"\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config not found at: {config_path}\")\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        pipeline_cfg = yaml.safe_load(f)\n",
    "\n",
    "    print(f\"‚úÖ Configuration loaded successfully from ‚Üí {config_path}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: pipeline_config.yml not found!\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading configuration: {e}\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# CONFIG CLASS\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        MODEL_TYPE = pipeline_cfg[\"model\"][\"type\"]\n",
    "        UC_CATALOG = pipeline_cfg[\"model\"][\"catalog\"]\n",
    "        UC_SCHEMA = pipeline_cfg[\"model\"][\"schema\"]\n",
    "        BASE_NAME = pipeline_cfg[\"model\"][\"base_name\"]\n",
    "        \n",
    "        self.MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{BASE_NAME}_{MODEL_TYPE}\"\n",
    "        self.MODEL_TYPE = MODEL_TYPE\n",
    "        \n",
    "        self.STAGING_ALIAS = pipeline_cfg[\"aliases\"][\"staging\"]\n",
    "        self.PRODUCTION_ALIAS = pipeline_cfg[\"aliases\"][\"production\"]\n",
    "        \n",
    "        self.UAT_INPUT_TABLE = pipeline_cfg[\"data\"][\"preprocessed_table\"]\n",
    "        self.LABEL_COL = \"label\"\n",
    "        \n",
    "        self.PRIMARY_METRIC = pipeline_cfg[\"metrics\"][\"classification\"][\"primary_metric\"]\n",
    "        self.DIRECTION = pipeline_cfg[\"metrics\"][\"classification\"][\"direction\"]\n",
    "        self.TRACKED_METRICS = pipeline_cfg[\"metrics\"][\"classification\"][\"tracked_metrics\"]\n",
    "        self.UAT_THRESHOLDS = pipeline_cfg[\"uat\"][\"classification_thresholds\"]\n",
    "        \n",
    "        self.UAT_RESULTS_TABLE = pipeline_cfg[\"tables\"][\"uat_results\"]\n",
    "        \n",
    "        self.SLACK_ENABLED = pipeline_cfg[\"notifications\"][\"enabled\"]\n",
    "        self.SLACK_WEBHOOK_URL = None\n",
    "\n",
    "        print(f\"\\nüìä Configuration Summary:\")\n",
    "        print(f\"   Model: {self.MODEL_NAME}\")\n",
    "        print(f\"   Alias: @{self.STAGING_ALIAS}\")\n",
    "        print(f\"   UAT Input: {self.UAT_INPUT_TABLE}\")\n",
    "        print(f\"   Primary Metric: {self.PRIMARY_METRIC}\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# SLACK NOTIFICATION (FROM V1) ‚Äî ADDED\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def get_slack_webhook():\n",
    "    for scope in [\"shared-scope\", \"dev-scope\"]:\n",
    "        try:\n",
    "            webhook = dbutils.secrets.get(scope, \"SLACK_WEBHOOK_URL\")\n",
    "            if webhook.strip():\n",
    "                print(f\"‚úì Slack webhook found from scope: {scope}\")\n",
    "                return webhook\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"‚ö† Slack webhook not found. Notifications disabled.\")\n",
    "    return None\n",
    "\n",
    "SLACK_WEBHOOK_URL = get_slack_webhook()\n",
    "\n",
    "def send_slack_notification(message, level=\"info\"):\n",
    "    if not SLACK_WEBHOOK_URL:\n",
    "        print(f\"üì¢ Slack Disabled ‚Äî {message}\")\n",
    "        return\n",
    "\n",
    "    emoji = {\"info\":\"‚ÑπÔ∏è\",\"success\":\"‚úÖ\",\"warning\":\"‚ö†Ô∏è\",\"error\":\"‚ùå\"}.get(level, \"‚ÑπÔ∏è\")\n",
    "    payload = {\"text\": f\"{emoji} {message}\"}\n",
    "\n",
    "    try:\n",
    "        r = requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=5)\n",
    "        print(\"üì® Slack Notification Sent\" if r.status_code == 200 else f\"‚ö† Slack Error: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Slack send failed: {e}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# SPARK + MLFLOW INIT\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nüîß Step 2: Initializing MLflow and Spark...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"UAT_Inference\").getOrCreate()\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(\"‚úÖ MLflow and Spark initialized successfully\")\n",
    "    send_slack_notification(\"üöÄ UAT Pipeline Started\", \"info\")\n",
    "\n",
    "except Exception as e:\n",
    "    send_slack_notification(f\"‚ùå MLflow/Spark Init Failed: {e}\", \"error\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def vector_to_array(v):\n",
    "    return v.toArray() if hasattr(v, 'toArray') else np.array(v)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 1: LOAD MODEL\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def load_staging_model():\n",
    "    print(\"\\nüìç Loading model from Unity Catalog...\")\n",
    "    \n",
    "    model_version = client.get_model_version_by_alias(config.MODEL_NAME, config.STAGING_ALIAS)\n",
    "    model_uri = f\"models:/{config.MODEL_NAME}@{config.STAGING_ALIAS}\"\n",
    "\n",
    "    print(f\"üî• Loaded Version: {model_version.version}\")    \n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "    send_slack_notification(f\"üì¶ Model Loaded: {config.MODEL_NAME} v{model_version.version}\", \"info\")\n",
    "\n",
    "    return model, int(model_version.version), model_version.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 2: LOAD UAT DATA\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def load_uat_data():\n",
    "    print(\"\\nüìç Loading UAT Dataset...\")\n",
    "\n",
    "    df_spark = spark.read.format(\"delta\").table(config.UAT_INPUT_TABLE)\n",
    "    df = df_spark.toPandas()\n",
    "\n",
    "    X = np.array([vector_to_array(r) for r in df[\"features\"]])\n",
    "    y_true = df[config.LABEL_COL].values\n",
    "    \n",
    "    return df, X, y_true\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 3: RUN INFERENCE\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def run_inference(model, X):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    try:\n",
    "        y_pred_proba = model._model_impl.predict_proba(X)[:, 1]\n",
    "    except:\n",
    "        y_pred_proba = y_pred.astype(float)\n",
    "\n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 4: CALCULATE METRICS\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred_proba)\n",
    "    except:\n",
    "        metrics[\"roc_auc\"] = None\n",
    "\n",
    "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 5: VALIDATE UAT\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def validate_uat(metrics, version):\n",
    "    failed = []\n",
    "\n",
    "    for key, min_val in config.UAT_THRESHOLDS.items():\n",
    "        if key.startswith(\"min_\"):\n",
    "            metric_name = key.replace(\"min_\", \"\")\n",
    "            if metrics.get(metric_name, 0) < min_val:\n",
    "                failed.append(key)\n",
    "\n",
    "    status = \"PASSED\" if len(failed) == 0 else \"FAILED\"\n",
    "\n",
    "    if status == \"PASSED\":\n",
    "        send_slack_notification(f\"üéâ Model PASSED UAT ‚Äî v{version}\", \"success\")\n",
    "    else:\n",
    "        send_slack_notification(f\"‚ùå Model FAILED UAT ‚Äî v{version}\", \"error\")\n",
    "\n",
    "    return status, failed\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# STEP 6: STORE RESULTS\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def log_results(version, run_id, metrics, status, failed_checks):\n",
    "\n",
    "    print(\"\\nüìç Logging UAT Results into Delta Table...\")\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    result = {\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"model_type\": config.MODEL_TYPE,\n",
    "        \"model_version\": str(version),\n",
    "        \"run_id\": run_id,\n",
    "        \"uat_status\": status,\n",
    "        \"accuracy\": float(metrics.get(\"accuracy\", 0)),\n",
    "        \"precision\": float(metrics.get(\"precision\", 0)),\n",
    "        \"recall\": float(metrics.get(\"recall\", 0)),\n",
    "        \"f1\": float(metrics.get(\"f1\", 0)),\n",
    "        \"roc_auc\": float(metrics.get(\"roc_auc\", 0)) if metrics.get(\"roc_auc\") else None,\n",
    "        \"confusion_matrix_json\": json.dumps(metrics.get(\"confusion_matrix\", [])),\n",
    "        \"failed_checks_json\": json.dumps(failed_checks) if failed_checks else None\n",
    "    }\n",
    "\n",
    "    df = spark.createDataFrame(pd.DataFrame([result]))\n",
    "\n",
    "    try:\n",
    "        DeltaTable.forName(spark, config.UAT_RESULTS_TABLE)\n",
    "        df.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(config.UAT_RESULTS_TABLE)\n",
    "    except:\n",
    "        df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(config.UAT_RESULTS_TABLE)\n",
    "\n",
    "    print(\"üìå Results logged.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        model, version, run_id = load_staging_model()\n",
    "        df, X, y_true = load_uat_data()\n",
    "        y_pred, y_pred_proba = run_inference(model, X)\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_pred_proba)\n",
    "        status, failed_checks = validate_uat(metrics, version)\n",
    "        log_results(version, run_id, metrics, status, failed_checks)\n",
    "\n",
    "        print(\"\\nüéØ Final UAT Status:\", status)\n",
    "\n",
    "    except Exception as e:\n",
    "        send_slack_notification(f\"üî• UAT Pipeline Failed ‚Äî {e}\", \"error\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
